{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62c9b761-74f0-4e04-831d-dc7c1fe9bb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading dataset...\n",
      "Dataset loaded successfully!\n",
      "Train size: 1000000\n",
      "Validation size: 2000\n",
      "Test size: 2000\n",
      "\n",
      "Example translations from dataset:\n",
      "\n",
      "Example 1:\n",
      "English: The time now is 05:08 .\n",
      "French: The time now is 05:05 .\n",
      "\n",
      "Example 2:\n",
      "English: This Regulation shall enter into force on the seventh day following its publication in the Official Journal of the European Union.\n",
      "French: Le présent règlement entre en vigueur le septième jour suivant celui de sa publication au Journal officiel de l'Union européenne.\n",
      "\n",
      "Example 3:\n",
      "English: Hello, what's that?\n",
      "French: Qu'est-ce que c'est que ça ?\n",
      "\n",
      "Creating tokenizers...\n",
      "Total English texts: 1000000\n",
      "Total French texts: 1000000\n",
      "\n",
      "Initializing en tokenizer...\n",
      "Building vocabulary for en...\n",
      "Processing text 0/1000000\n",
      "Processing text 10000/1000000\n",
      "Processing text 20000/1000000\n",
      "Processing text 30000/1000000\n",
      "Processing text 40000/1000000\n",
      "Processing text 50000/1000000\n",
      "Processing text 60000/1000000\n",
      "Processing text 70000/1000000\n",
      "Processing text 80000/1000000\n",
      "Processing text 90000/1000000\n",
      "Processing text 100000/1000000\n",
      "Processing text 110000/1000000\n",
      "Processing text 120000/1000000\n",
      "Processing text 130000/1000000\n",
      "Processing text 140000/1000000\n",
      "Processing text 150000/1000000\n",
      "Processing text 160000/1000000\n",
      "Processing text 170000/1000000\n",
      "Processing text 180000/1000000\n",
      "Processing text 190000/1000000\n",
      "Processing text 200000/1000000\n",
      "Processing text 210000/1000000\n",
      "Processing text 220000/1000000\n",
      "Processing text 230000/1000000\n",
      "Processing text 240000/1000000\n",
      "Processing text 250000/1000000\n",
      "Processing text 260000/1000000\n",
      "Processing text 270000/1000000\n",
      "Processing text 280000/1000000\n",
      "Processing text 290000/1000000\n",
      "Processing text 300000/1000000\n",
      "Processing text 310000/1000000\n",
      "Processing text 320000/1000000\n",
      "Processing text 330000/1000000\n",
      "Processing text 340000/1000000\n",
      "Processing text 350000/1000000\n",
      "Processing text 360000/1000000\n",
      "Processing text 370000/1000000\n",
      "Processing text 380000/1000000\n",
      "Processing text 390000/1000000\n",
      "Processing text 400000/1000000\n",
      "Processing text 410000/1000000\n",
      "Processing text 420000/1000000\n",
      "Processing text 430000/1000000\n",
      "Processing text 440000/1000000\n",
      "Processing text 450000/1000000\n",
      "Processing text 460000/1000000\n",
      "Processing text 470000/1000000\n",
      "Processing text 480000/1000000\n",
      "Processing text 490000/1000000\n",
      "Processing text 500000/1000000\n",
      "Processing text 510000/1000000\n",
      "Processing text 520000/1000000\n",
      "Processing text 530000/1000000\n",
      "Processing text 540000/1000000\n",
      "Processing text 550000/1000000\n",
      "Processing text 560000/1000000\n",
      "Processing text 570000/1000000\n",
      "Processing text 580000/1000000\n",
      "Processing text 590000/1000000\n",
      "Processing text 600000/1000000\n",
      "Processing text 610000/1000000\n",
      "Processing text 620000/1000000\n",
      "Processing text 630000/1000000\n",
      "Processing text 640000/1000000\n",
      "Processing text 650000/1000000\n",
      "Processing text 660000/1000000\n",
      "Processing text 670000/1000000\n",
      "Processing text 680000/1000000\n",
      "Processing text 690000/1000000\n",
      "Processing text 700000/1000000\n",
      "Processing text 710000/1000000\n",
      "Processing text 720000/1000000\n",
      "Processing text 730000/1000000\n",
      "Processing text 740000/1000000\n",
      "Processing text 750000/1000000\n",
      "Processing text 760000/1000000\n",
      "Processing text 770000/1000000\n",
      "Processing text 780000/1000000\n",
      "Processing text 790000/1000000\n",
      "Processing text 800000/1000000\n",
      "Processing text 810000/1000000\n",
      "Processing text 820000/1000000\n",
      "Processing text 830000/1000000\n",
      "Processing text 840000/1000000\n",
      "Processing text 850000/1000000\n",
      "Processing text 860000/1000000\n",
      "Processing text 870000/1000000\n",
      "Processing text 880000/1000000\n",
      "Processing text 890000/1000000\n",
      "Processing text 900000/1000000\n",
      "Processing text 910000/1000000\n",
      "Processing text 920000/1000000\n",
      "Processing text 930000/1000000\n",
      "Processing text 940000/1000000\n",
      "Processing text 950000/1000000\n",
      "Processing text 960000/1000000\n",
      "Processing text 970000/1000000\n",
      "Processing text 980000/1000000\n",
      "Processing text 990000/1000000\n",
      "Vocabulary size for en: 50000\n",
      "Sample of most frequent words in en:\n",
      "  the: 953883\n",
      "  of: 528273\n",
      "  and: 412106\n",
      "  to: 400370\n",
      "  in: 294901\n",
      "  a: 246277\n",
      "  for: 157116\n",
      "  is: 136440\n",
      "  that: 133969\n",
      "  you: 119591\n",
      "\n",
      "Initializing fr tokenizer...\n",
      "Building vocabulary for fr...\n",
      "Processing text 0/1000000\n",
      "Processing text 10000/1000000\n",
      "Processing text 20000/1000000\n",
      "Processing text 30000/1000000\n",
      "Processing text 40000/1000000\n",
      "Processing text 50000/1000000\n",
      "Processing text 60000/1000000\n",
      "Processing text 70000/1000000\n",
      "Processing text 80000/1000000\n",
      "Processing text 90000/1000000\n",
      "Processing text 100000/1000000\n",
      "Processing text 110000/1000000\n",
      "Processing text 120000/1000000\n",
      "Processing text 130000/1000000\n",
      "Processing text 140000/1000000\n",
      "Processing text 150000/1000000\n",
      "Processing text 160000/1000000\n",
      "Processing text 170000/1000000\n",
      "Processing text 180000/1000000\n",
      "Processing text 190000/1000000\n",
      "Processing text 200000/1000000\n",
      "Processing text 210000/1000000\n",
      "Processing text 220000/1000000\n",
      "Processing text 230000/1000000\n",
      "Processing text 240000/1000000\n",
      "Processing text 250000/1000000\n",
      "Processing text 260000/1000000\n",
      "Processing text 270000/1000000\n",
      "Processing text 280000/1000000\n",
      "Processing text 290000/1000000\n",
      "Processing text 300000/1000000\n",
      "Processing text 310000/1000000\n",
      "Processing text 320000/1000000\n",
      "Processing text 330000/1000000\n",
      "Processing text 340000/1000000\n",
      "Processing text 350000/1000000\n",
      "Processing text 360000/1000000\n",
      "Processing text 370000/1000000\n",
      "Processing text 380000/1000000\n",
      "Processing text 390000/1000000\n",
      "Processing text 400000/1000000\n",
      "Processing text 410000/1000000\n",
      "Processing text 420000/1000000\n",
      "Processing text 430000/1000000\n",
      "Processing text 440000/1000000\n",
      "Processing text 450000/1000000\n",
      "Processing text 460000/1000000\n",
      "Processing text 470000/1000000\n",
      "Processing text 480000/1000000\n",
      "Processing text 490000/1000000\n",
      "Processing text 500000/1000000\n",
      "Processing text 510000/1000000\n",
      "Processing text 520000/1000000\n",
      "Processing text 530000/1000000\n",
      "Processing text 540000/1000000\n",
      "Processing text 550000/1000000\n",
      "Processing text 560000/1000000\n",
      "Processing text 570000/1000000\n",
      "Processing text 580000/1000000\n",
      "Processing text 590000/1000000\n",
      "Processing text 600000/1000000\n",
      "Processing text 610000/1000000\n",
      "Processing text 620000/1000000\n",
      "Processing text 630000/1000000\n",
      "Processing text 640000/1000000\n",
      "Processing text 650000/1000000\n",
      "Processing text 660000/1000000\n",
      "Processing text 670000/1000000\n",
      "Processing text 680000/1000000\n",
      "Processing text 690000/1000000\n",
      "Processing text 700000/1000000\n",
      "Processing text 710000/1000000\n",
      "Processing text 720000/1000000\n",
      "Processing text 730000/1000000\n",
      "Processing text 740000/1000000\n",
      "Processing text 750000/1000000\n",
      "Processing text 760000/1000000\n",
      "Processing text 770000/1000000\n",
      "Processing text 780000/1000000\n",
      "Processing text 790000/1000000\n",
      "Processing text 800000/1000000\n",
      "Processing text 810000/1000000\n",
      "Processing text 820000/1000000\n",
      "Processing text 830000/1000000\n",
      "Processing text 840000/1000000\n",
      "Processing text 850000/1000000\n",
      "Processing text 860000/1000000\n",
      "Processing text 870000/1000000\n",
      "Processing text 880000/1000000\n",
      "Processing text 890000/1000000\n",
      "Processing text 900000/1000000\n",
      "Processing text 910000/1000000\n",
      "Processing text 920000/1000000\n",
      "Processing text 930000/1000000\n",
      "Processing text 940000/1000000\n",
      "Processing text 950000/1000000\n",
      "Processing text 960000/1000000\n",
      "Processing text 970000/1000000\n",
      "Processing text 980000/1000000\n",
      "Processing text 990000/1000000\n",
      "Vocabulary size for fr: 50000\n",
      "Sample of most frequent words in fr:\n",
      "  de: 887940\n",
      "  la: 457195\n",
      "  et: 376439\n",
      "  des: 334185\n",
      "  les: 303589\n",
      "  à: 301141\n",
      "  le: 289460\n",
      "  du: 203127\n",
      "  en: 186378\n",
      "  que: 153296\n",
      "\n",
      "Tokenization examples:\n",
      "\n",
      "Example 1:\n",
      "English: The time now is 05:08 .\n",
      "Tokenized English: [2, 4, 96, 149, 11, 0, 172, 3]\n",
      "Decoded English: the time now is <unk> .\n",
      "French: The time now is 05:05 .\n",
      "Tokenized French: [2, 1035, 6327, 6869, 2256, 0, 167, 3]\n",
      "Decoded French: The time now is <unk> .\n",
      "\n",
      "Example 2:\n",
      "English: This Regulation shall enter into force on the seventh day following its publication in the Official Journal of the European Union.\n",
      "Tokenized English: [2, 20, 242, 90, 1229, 82, 440, 14, 4, 5955, 299, 176, 40, 1745, 8, 4, 447, 2039, 5, 4, 85, 2236, 3]\n",
      "Decoded English: this regulation shall enter into force on the seventh day following its publication in the official journal of the european union.\n",
      "French: Le présent règlement entre en vigueur le septième jour suivant celui de sa publication au Journal officiel de l'Union européenne.\n",
      "Tokenized French: [2, 32, 296, 229, 73, 12, 662, 10, 6651, 205, 897, 463, 4, 67, 1298, 19, 2305, 1769, 4, 381, 1576, 3]\n",
      "Decoded French: Le présent règlement entre en vigueur le septième jour suivant celui de sa publication au Journal officiel de l'Union européenne.\n",
      "\n",
      "Example 3:\n",
      "English: Hello, what's that?\n",
      "Tokenized English: [2, 2778, 349, 725, 3]\n",
      "Decoded English: hello, what's that?\n",
      "French: Qu'est-ce que c'est que ça ?\n",
      "Tokenized French: [2, 423, 13, 77, 13, 91, 26, 3]\n",
      "Decoded French: Qu'est-ce que c'est que ça ?\n",
      "\n",
      "Creating datasets and dataloaders...\n",
      "\n",
      "Creating dataset with 1000000 examples...\n",
      "\n",
      "Creating dataset with 2000 examples...\n",
      "\n",
      "Creating dataset with 2000 examples...\n",
      "\n",
      "Creating dataloaders with batch size 8\n",
      "Number of training batches: 125000\n",
      "Number of validation batches: 250\n",
      "Number of test batches: 250\n",
      "\n",
      "Initializing model hyperparameters...\n",
      "Input dimension: 50000\n",
      "Output dimension: 50000\n",
      "\n",
      "Training hyperparameters:\n",
      "Number of epochs: 5\n",
      "Gradient clipping: 1\n",
      "Learning rate: 0.0001\n",
      "\n",
      "=== Training Phase ===\n",
      "\n",
      "Training with multiplicative attention...\n",
      "Initializing encoder and decoder...\n",
      "Creating Seq2SeqTransformer model...\n",
      "Model parameters: 4904192\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/5\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "943a2a24b68e49bcb9f157309f73473f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|                              | 0/125000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf0bb1e8abca4ceaaffe3be0b082b8af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|                              | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best validation loss: 5.9038\n",
      "Saving model to en-fr-transformer-multiplicative.pt\n",
      "Epoch: 01\n",
      "Train Loss: 1.603 | Train PPL:   4.966\n",
      "Val. Loss: 5.904 | Val. PPL: 366.420\n",
      "\n",
      "Epoch 2/5\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb9a57572b748d093bd24befb448c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|                              | 0/125000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a8769cee3c54a1f9a9d216ce25e26bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|                              | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best validation loss: 5.5352\n",
      "Saving model to en-fr-transformer-multiplicative.pt\n",
      "Epoch: 02\n",
      "Train Loss: 1.425 | Train PPL:   4.157\n",
      "Val. Loss: 5.535 | Val. PPL: 253.459\n",
      "\n",
      "Epoch 3/5\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d172a68ab9ba46b0ba971eeb5ce7fada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|                              | 0/125000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fdb5c5d01c4494d81d3efcf04b58573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|                              | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best validation loss: 5.3269\n",
      "Saving model to en-fr-transformer-multiplicative.pt\n",
      "Epoch: 03\n",
      "Train Loss: 1.356 | Train PPL:   3.882\n",
      "Val. Loss: 5.327 | Val. PPL: 205.800\n",
      "\n",
      "Epoch 4/5\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4bf541e3cb4c8f80aadd250c5e4ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|                              | 0/125000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b732ca39cc740b89b24a052dbff8ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|                              | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best validation loss: 5.1851\n",
      "Saving model to en-fr-transformer-multiplicative.pt\n",
      "Epoch: 04\n",
      "Train Loss: 1.313 | Train PPL:   3.718\n",
      "Val. Loss: 5.185 | Val. PPL: 178.583\n",
      "\n",
      "Epoch 5/5\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7790fe1e31724766b48c5e4e10d31233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|                              | 0/125000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b5da2af6f774c5cab298d17fe29e4a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|                              | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best validation loss: 5.0744\n",
      "Saving model to en-fr-transformer-multiplicative.pt\n",
      "Epoch: 05\n",
      "Train Loss: 1.282 | Train PPL:   3.605\n",
      "Val. Loss: 5.074 | Val. PPL: 159.882\n",
      "\n",
      "Training with general attention...\n",
      "Initializing encoder and decoder...\n",
      "Creating Seq2SeqTransformer model...\n",
      "Model parameters: 4903376\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/5\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2a26143cfd4d5dbcfc2b17c8f0ad6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|                              | 0/125000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec13af448ded490c80061d324faeef03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|                              | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best validation loss: 5.9449\n",
      "Saving model to en-fr-transformer-general.pt\n",
      "Epoch: 01\n",
      "Train Loss: 1.606 | Train PPL:   4.985\n",
      "Val. Loss: 5.945 | Val. PPL: 381.815\n",
      "\n",
      "Epoch 2/5\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52bf996cedce479a942e8b6cfbbdae08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|                              | 0/125000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca37207cc224cff85ce2a7f7188afdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|                              | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best validation loss: 5.6080\n",
      "Saving model to en-fr-transformer-general.pt\n",
      "Epoch: 02\n",
      "Train Loss: 1.440 | Train PPL:   4.219\n",
      "Val. Loss: 5.608 | Val. PPL: 272.590\n",
      "\n",
      "Epoch 3/5\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e60b72dce04c96af9dec9ca248593d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|                              | 0/125000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0efe3a56333478da743a151b5582976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|                              | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best validation loss: 5.3927\n",
      "Saving model to en-fr-transformer-general.pt\n",
      "Epoch: 03\n",
      "Train Loss: 1.374 | Train PPL:   3.952\n",
      "Val. Loss: 5.393 | Val. PPL: 219.786\n",
      "\n",
      "Epoch 4/5\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c7a689ff7d4470e8cd69ff2e8e0cbbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|                              | 0/125000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9489934e84454aecbe473f1bd77e5b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|                              | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best validation loss: 5.2355\n",
      "Saving model to en-fr-transformer-general.pt\n",
      "Epoch: 04\n",
      "Train Loss: 1.328 | Train PPL:   3.773\n",
      "Val. Loss: 5.235 | Val. PPL: 187.819\n",
      "\n",
      "Epoch 5/5\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06f04b2296ce41bfbfd4baa2504aa906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|                              | 0/125000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0370486bd704397b13fc1eebb7cf050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|                              | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best validation loss: 5.1221\n",
      "Saving model to en-fr-transformer-general.pt\n",
      "Epoch: 05\n",
      "Train Loss: 1.294 | Train PPL:   3.648\n",
      "Val. Loss: 5.122 | Val. PPL: 167.683\n",
      "\n",
      "Training with additive attention...\n",
      "Initializing encoder and decoder...\n",
      "Creating Seq2SeqTransformer model...\n",
      "Model parameters: 4905059\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/5\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2e7be019714c3f8c5e4b9465662985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|                              | 0/125000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "value cannot be converted to type at::Half without overflow",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 734\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 734\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    737\u001b[0m valid_loss \u001b[38;5;241m=\u001b[39m evaluate(model, valid_loader, criterion)\n",
      "Cell \u001b[0;32mIn[11], line 727\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCLIP\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    729\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout of memory\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "Cell \u001b[0;32mIn[11], line 389\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m    386\u001b[0m trg \u001b[38;5;241m=\u001b[39m trg\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[0;32m--> 389\u001b[0m     output, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m     output_dim \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    392\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output_dim)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 364\u001b[0m, in \u001b[0;36mSeq2SeqTransformer.forward\u001b[0;34m(self, src, trg)\u001b[0m\n\u001b[1;32m    362\u001b[0m src_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_src_mask(src)\n\u001b[1;32m    363\u001b[0m trg_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_trg_mask(trg)\n\u001b[0;32m--> 364\u001b[0m enc_src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m output, attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(trg, enc_src, trg_mask, src_mask)\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, attention\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 287\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, src, src_mask)\u001b[0m\n\u001b[1;32m    284\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtok_embedding(src) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding(pos))\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 287\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m src\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 261\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[0;34m(self, src, src_mask)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, src_mask):\n\u001b[0;32m--> 261\u001b[0m     _src, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn_layer_norm(src \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(_src))\n\u001b[1;32m    263\u001b[0m     _src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositionwise_feedforward(src)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 234\u001b[0m, in \u001b[0;36mMultiHeadAttentionLayer.forward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m    231\u001b[0m     energy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mV(energy)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [batch, heads, query_len, key_len]\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m     energy \u001b[38;5;241m=\u001b[39m \u001b[43menergy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1e10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m attention \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(energy, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    237\u001b[0m attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: value cannot be converted to type at::Half without overflow"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "from sacrebleu.metrics import BLEU\n",
    "from tqdm.auto import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Constants for special tokens\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
    "\n",
    "# Language constants\n",
    "SRC_LANGUAGE = 'en'\n",
    "TRG_LANGUAGE = 'fr'\n",
    "\n",
    "class CustomTokenizer:\n",
    "    def __init__(self, texts, max_vocab_size=50000, language='en'):\n",
    "        print(f\"\\nInitializing {language} tokenizer...\")\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.language = language\n",
    "        self.word2idx = {'<unk>': UNK_IDX, '<pad>': PAD_IDX, '<sos>': SOS_IDX, '<eos>': EOS_IDX}\n",
    "        self.idx2word = {v: k for k, v in self.word2idx.items()}\n",
    "        self.vocab_size = len(special_symbols)\n",
    "        \n",
    "        print(f\"Building vocabulary for {language}...\")\n",
    "        # Build vocabulary\n",
    "        word_freq = Counter()\n",
    "        for i, text in enumerate(texts):\n",
    "            if i % 10000 == 0:\n",
    "                print(f\"Processing text {i}/{len(texts)}\")\n",
    "            \n",
    "            # Apply language-specific normalization\n",
    "            if language == 'fr':\n",
    "                text = unicodedata.normalize('NFKC', text)\n",
    "            else:\n",
    "                text = text.lower()\n",
    "            \n",
    "            words = text.split()\n",
    "            word_freq.update(words)\n",
    "        \n",
    "        # Add most common words to vocabulary\n",
    "        for word, freq in word_freq.most_common(max_vocab_size - len(special_symbols)):\n",
    "            if word not in self.word2idx:\n",
    "                self.word2idx[word] = self.vocab_size\n",
    "                self.idx2word[self.vocab_size] = word\n",
    "                self.vocab_size += 1\n",
    "        \n",
    "        print(f\"Vocabulary size for {language}: {self.vocab_size}\")\n",
    "        print(f\"Sample of most frequent words in {language}:\")\n",
    "        for word, freq in list(word_freq.most_common(10)):\n",
    "            print(f\"  {word}: {freq}\")\n",
    "    \n",
    "    def encode(self, text):\n",
    "        if self.language == 'fr':\n",
    "            text = unicodedata.normalize('NFKC', text)\n",
    "        else:\n",
    "            text = text.lower()\n",
    "        words = text.split()\n",
    "        return [SOS_IDX] + [self.word2idx.get(word, UNK_IDX) for word in words] + [EOS_IDX]\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        return ' '.join([self.idx2word.get(idx, '<unk>') for idx in indices if idx not in [PAD_IDX, SOS_IDX, EOS_IDX]])\n",
    "\n",
    "# Load the English-French dataset\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"opus100\", \"en-fr\")\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Train size: {len(dataset['train'])}\")\n",
    "print(f\"Validation size: {len(dataset['validation'])}\")\n",
    "print(f\"Test size: {len(dataset['test'])}\")\n",
    "\n",
    "# Print some examples\n",
    "print(\"\\nExample translations from dataset:\")\n",
    "for i in range(3):\n",
    "    example = dataset['train'][i]\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"English: {example['translation']['en']}\")\n",
    "    print(f\"French: {example['translation']['fr']}\")\n",
    "\n",
    "print(\"\\nCreating tokenizers...\")\n",
    "src_texts = [example['translation']['en'] for example in dataset['train']]\n",
    "trg_texts = [example['translation']['fr'] for example in dataset['train']]\n",
    "\n",
    "print(f\"Total English texts: {len(src_texts)}\")\n",
    "print(f\"Total French texts: {len(trg_texts)}\")\n",
    "\n",
    "src_tokenizer = CustomTokenizer(src_texts, language='en')\n",
    "trg_tokenizer = CustomTokenizer(trg_texts, language='fr')\n",
    "\n",
    "# Add tokenization examples\n",
    "print(\"\\nTokenization examples:\")\n",
    "for i in range(3):\n",
    "    example = dataset['train'][i]\n",
    "    en_text = example['translation']['en']\n",
    "    fr_text = example['translation']['fr']\n",
    "    \n",
    "    en_tokens = src_tokenizer.encode(en_text)\n",
    "    fr_tokens = trg_tokenizer.encode(fr_text)\n",
    "    \n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"English: {en_text}\")\n",
    "    print(f\"Tokenized English: {en_tokens}\")\n",
    "    print(f\"Decoded English: {src_tokenizer.decode(en_tokens)}\")\n",
    "    print(f\"French: {fr_text}\")\n",
    "    print(f\"Tokenized French: {fr_tokens}\")\n",
    "    print(f\"Decoded French: {trg_tokenizer.decode(fr_tokens)}\")\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, dataset_split, src_tokenizer, trg_tokenizer, max_len=128):\n",
    "        print(f\"\\nCreating dataset with {len(dataset_split)} examples...\")\n",
    "        self.examples = dataset_split\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.trg_tokenizer = trg_tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        src_text = example['translation']['en']\n",
    "        trg_text = example['translation']['fr']\n",
    "        \n",
    "        src_tokens = self.src_tokenizer.encode(src_text)[:self.max_len]\n",
    "        trg_tokens = self.trg_tokenizer.encode(trg_text)[:self.max_len]\n",
    "        \n",
    "        return torch.tensor(src_tokens), torch.tensor(trg_tokens)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for batching sequences of different lengths.\n",
    "    Pads sequences to the maximum length in the batch.\n",
    "    \"\"\"\n",
    "    src_batch, trg_batch = [], []\n",
    "    for src_sample, trg_sample in batch:\n",
    "        src_batch.append(src_sample)\n",
    "        trg_batch.append(trg_sample)\n",
    "    \n",
    "    # Pad sequences to the maximum length in the batch\n",
    "    src_batch = nn.utils.rnn.pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    trg_batch = nn.utils.rnn.pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    \n",
    "    return src_batch, trg_batch\n",
    "\n",
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, attn_variant, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert hid_dim % n_heads == 0\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        self.attn_variant = attn_variant\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize layers based on attention variant\n",
    "        if attn_variant == 'multiplicative':\n",
    "            self.W = nn.Linear(self.head_dim, self.head_dim)\n",
    "        elif attn_variant == 'additive':\n",
    "            self.Wa = nn.Linear(self.head_dim, self.head_dim)\n",
    "            self.Ua = nn.Linear(self.head_dim, self.head_dim)\n",
    "            self.V = nn.Linear(self.head_dim, 1)\n",
    "        # General attention doesn't need additional parameters\n",
    "        \n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        \n",
    "        # Split into heads\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        # Calculate attention scores based on variant\n",
    "        if self.attn_variant == 'multiplicative':\n",
    "            # Multiplicative attention\n",
    "            K_transformed = self.W(K)\n",
    "            energy = torch.matmul(Q, K_transformed.transpose(-2, -1)) / self.scale\n",
    "            \n",
    "        elif self.attn_variant == 'general':\n",
    "            # General attention\n",
    "            energy = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "            \n",
    "        elif self.attn_variant == 'additive':\n",
    "            # Additive attention\n",
    "            Q_transformed = self.Wa(Q)\n",
    "            K_transformed = self.Ua(K)\n",
    "            \n",
    "            # Expand dimensions for broadcasting\n",
    "            Q_expanded = Q_transformed.unsqueeze(-2)  # [batch, heads, query_len, 1, head_dim]\n",
    "            K_expanded = K_transformed.unsqueeze(-3)  # [batch, heads, 1, key_len, head_dim]\n",
    "            \n",
    "            # Calculate additive attention\n",
    "            energy = torch.tanh(Q_expanded + K_expanded)  # [batch, heads, query_len, key_len, head_dim]\n",
    "            energy = self.V(energy).squeeze(-1)  # [batch, heads, query_len, key_len]\n",
    "        \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "        attention = self.dropout(attention)\n",
    "        \n",
    "        x = torch.matmul(attention, V)\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        x = self.fc_o(x)\n",
    "        \n",
    "        return x, attention\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, attn_variant, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, attn_variant, device)\n",
    "        self.positionwise_feedforward = nn.Sequential(\n",
    "            nn.Linear(hid_dim, pf_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(pf_dim, hid_dim)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        return src\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, attn_variant, device, max_length=500):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(hid_dim, n_heads, pf_dim, dropout, attn_variant, device)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        return src\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, attn_variant, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, attn_variant, device)\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, attn_variant, device)\n",
    "        self.positionwise_feedforward = nn.Sequential(\n",
    "            nn.Linear(hid_dim, pf_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(pf_dim, hid_dim)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        return trg, attention\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, attn_variant, device, max_length=500):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(hid_dim, n_heads, pf_dim, dropout, attn_variant, device)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "            \n",
    "        output = self.fc_out(trg)\n",
    "        return output, attention\n",
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask\n",
    "        \n",
    "    def make_trg_mask(self, trg):\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=self.device)).bool()\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        return output, attention\n",
    "\n",
    "# Gradient accumulation steps\n",
    "ACCUMULATION_STEPS = 4\n",
    "\n",
    "# Mixed precision training\n",
    "scaler = GradScaler()\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    total_batches = len(iterator)\n",
    "    \n",
    "    # Create progress bar\n",
    "    pbar = tqdm(iterator, total=total_batches, desc='Training', bar_format='{l_bar}{bar:30}{r_bar}')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for i, (src, trg) in enumerate(pbar):\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        with autocast():\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Perform optimization step after accumulating enough gradients\n",
    "        if (i + 1) % ACCUMULATION_STEPS == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        epoch_loss += loss.item() / ACCUMULATION_STEPS\n",
    "        \n",
    "        # Update progress bar description\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{epoch_loss / (i + 1):.4f}',\n",
    "            'ppl': f'{math.exp(epoch_loss / (i + 1)):.2f}'\n",
    "        })\n",
    "    \n",
    "    pbar.close()\n",
    "    return epoch_loss / total_batches\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    total_batches = len(iterator)\n",
    "    \n",
    "    # Create progress bar\n",
    "    pbar = tqdm(iterator, total=total_batches, desc='Evaluating', bar_format='{l_bar}{bar:30}{r_bar}')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, trg in pbar:\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                output, _ = model(src, trg[:,:-1])\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Update progress bar description\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "            'ppl': f'{math.exp(loss.item()):.2f}'\n",
    "        })\n",
    "    \n",
    "    pbar.close()\n",
    "    return epoch_loss / total_batches\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def visualize_attention(model, src_text, trg_text, src_tokenizer, trg_tokenizer, device, max_length=128):\n",
    "    \"\"\"\n",
    "    Visualize attention weights for a given source and target text pair.\n",
    "    Shows the attention map from the last decoder layer's first head.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Tokenize and encode texts\n",
    "        src_tokens = torch.tensor([src_tokenizer.encode(src_text)]).to(device)\n",
    "        trg_tokens = torch.tensor([trg_tokenizer.encode(trg_text)]).to(device)\n",
    "        \n",
    "        # Forward pass through the model\n",
    "        output, attention_weights = model(src_tokens, trg_tokens[:,:-1])\n",
    "        \n",
    "        # Get the last layer's attention weights (shape: [batch_size, n_heads, tgt_len, src_len])\n",
    "        last_layer_attention = attention_weights[-1]\n",
    "        \n",
    "        # Get first head's attention from first batch\n",
    "        attention = last_layer_attention[0, 0].cpu().numpy()\n",
    "        \n",
    "        # Get tokens for visualization\n",
    "        src_tokens_list = src_text.split()\n",
    "        trg_tokens_list = trg_text.split()\n",
    "        \n",
    "        # Get actual sequence lengths\n",
    "        src_len = len(src_tokens_list)\n",
    "        trg_len = len(trg_tokens_list)\n",
    "        \n",
    "        # Extract relevant part of attention matrix\n",
    "        attention_matrix = attention[:trg_len, :src_len]\n",
    "        \n",
    "        # Create figure with larger size\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Create heatmap with improved visibility\n",
    "        sns.heatmap(\n",
    "            attention_matrix,\n",
    "            xticklabels=src_tokens_list,\n",
    "            yticklabels=trg_tokens_list,\n",
    "            cmap='viridis',\n",
    "            annot=True,\n",
    "            fmt='.2f',\n",
    "            square=True,\n",
    "            cbar_kws={'label': 'Attention Weight'}\n",
    "        )\n",
    "        \n",
    "        # Rotate x-axis labels for better readability\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        \n",
    "        plt.title(f'Attention Weights Visualization\\n{model.attention_type} Attention', pad=20)\n",
    "        plt.xlabel('Source Text (English)', labelpad=10)\n",
    "        plt.ylabel('Target Text (French)', labelpad=10)\n",
    "        \n",
    "        # Adjust layout to prevent label cutoff\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save with high quality\n",
    "        filename = f'attention_map_{model.attention_type}_{src_text[:20].replace(\" \", \"_\")}.png'\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Saved attention map to: {filename}\")\n",
    "        \n",
    "        # Print attention weights for verification\n",
    "        print(\"\\nAttention Matrix Shape:\", attention_matrix.shape)\n",
    "        print(\"Attention Weights:\")\n",
    "        for i, trg_token in enumerate(trg_tokens_list):\n",
    "            print(f\"{trg_token:>20}: \", end=\"\")\n",
    "            for j, src_token in enumerate(src_tokens_list):\n",
    "                print(f\"{src_token}({attention_matrix[i,j]:.2f}) \", end=\"\")\n",
    "            print()\n",
    "\n",
    "def calculate_bleu(model, data_loader, src_tokenizer, trg_tokenizer):\n",
    "    \"\"\"\n",
    "    Calculate BLEU score for the model predictions.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    bleu = BLEU()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, trg in data_loader:\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                output, _ = model(src, trg[:,:-1])\n",
    "            \n",
    "            # Convert predictions to text\n",
    "            pred_tokens = output.argmax(dim=-1)\n",
    "            for pred, ref in zip(pred_tokens, trg):\n",
    "                pred_text = trg_tokenizer.decode(pred.cpu().numpy())\n",
    "                ref_text = trg_tokenizer.decode(ref.cpu().numpy())\n",
    "                predictions.append(pred_text)\n",
    "                references.append([ref_text])\n",
    "    \n",
    "    return bleu.corpus_score(predictions, references).score\n",
    "\n",
    "def translate_sentence(model, sentence, src_tokenizer, trg_tokenizer, device, max_length=128):\n",
    "    \"\"\"\n",
    "    Translate a single English sentence to French.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize and encode the source sentence\n",
    "    src_tokens = torch.tensor([src_tokenizer.encode(sentence)]).to(device)\n",
    "    \n",
    "    # Initialize target sequence with <sos>\n",
    "    trg_tokens = torch.tensor([[SOS_IDX]]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            with autocast():\n",
    "                # Get model prediction\n",
    "                output, _ = model(src_tokens, trg_tokens)\n",
    "            \n",
    "            # Get the next token prediction\n",
    "            pred_token = output.argmax(2)[:, -1].item()\n",
    "            \n",
    "            # Add predicted token to target sequence\n",
    "            trg_tokens = torch.cat([trg_tokens, torch.tensor([[pred_token]]).to(device)], dim=1)\n",
    "            \n",
    "            # Stop if <eos> is predicted\n",
    "            if pred_token == EOS_IDX:\n",
    "                break\n",
    "    \n",
    "    # Convert tokens back to text\n",
    "    translated_text = trg_tokenizer.decode(trg_tokens.squeeze().cpu().numpy())\n",
    "    return translated_text\n",
    "\n",
    "def evaluate_translations(model, test_loader, src_tokenizer, trg_tokenizer, device, num_examples=5):\n",
    "    \"\"\"\n",
    "    Evaluate model translations on test set examples.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    translations = []\n",
    "    \n",
    "    print(\"\\nEvaluating translations on test set examples:\")\n",
    "    with torch.no_grad():\n",
    "        for src, trg in test_loader:\n",
    "            if len(translations) >= num_examples:\n",
    "                break\n",
    "                \n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "            \n",
    "            # Get source and target texts\n",
    "            for i in range(src.size(0)):\n",
    "                if len(translations) >= num_examples:\n",
    "                    break\n",
    "                    \n",
    "                src_text = src_tokenizer.decode(src[i].cpu().numpy())\n",
    "                true_text = trg_tokenizer.decode(trg[i].cpu().numpy())\n",
    "                \n",
    "                # Get model translation\n",
    "                pred_text = translate_sentence(model, src_text, src_tokenizer, trg_tokenizer, device)\n",
    "                \n",
    "                translations.append({\n",
    "                    'source': src_text,\n",
    "                    'target': true_text,\n",
    "                    'prediction': pred_text\n",
    "                })\n",
    "    \n",
    "    return translations\n",
    "\n",
    "def test_custom_translations(model, src_tokenizer, trg_tokenizer, device):\n",
    "    \"\"\"\n",
    "    Test model on custom English sentences.\n",
    "    \"\"\"\n",
    "    test_sentences = [\n",
    "        \"How are you?\",\n",
    "        \"What is your name?\",\n",
    "        \"I love learning new languages.\",\n",
    "        \"The weather is beautiful today.\",\n",
    "        \"Thank you very much.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nTesting custom translations:\")\n",
    "    for sentence in test_sentences:\n",
    "        translation = translate_sentence(model, sentence, src_tokenizer, trg_tokenizer, device)\n",
    "        print(f\"\\nEnglish: {sentence}\")\n",
    "        print(f\"French: {translation}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nCreating datasets and dataloaders...\")\n",
    "    # Create datasets\n",
    "    train_dataset = TranslationDataset(dataset['train'], src_tokenizer, trg_tokenizer)\n",
    "    valid_dataset = TranslationDataset(dataset['validation'], src_tokenizer, trg_tokenizer)\n",
    "    test_dataset = TranslationDataset(dataset['test'], src_tokenizer, trg_tokenizer)\n",
    "    \n",
    "    # Create data loaders\n",
    "    BATCH_SIZE = 8  # Reduced batch size to 8\n",
    "    print(f\"\\nCreating dataloaders with batch size {BATCH_SIZE}\")\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    print(f\"Number of training batches: {len(train_loader)}\")\n",
    "    print(f\"Number of validation batches: {len(valid_loader)}\")\n",
    "    print(f\"Number of test batches: {len(test_loader)}\")\n",
    "    \n",
    "    # Model hyperparameters\n",
    "    print(\"\\nInitializing model hyperparameters...\")\n",
    "    INPUT_DIM = src_tokenizer.vocab_size\n",
    "    OUTPUT_DIM = trg_tokenizer.vocab_size\n",
    "    HID_DIM = 32  # Further reduced hidden dimension\n",
    "    ENC_LAYERS = 1  # Reduced number of encoder layers\n",
    "    DEC_LAYERS = 1  # Reduced number of decoder layers\n",
    "    ENC_HEADS = 2  # Reduced number of encoder heads\n",
    "    DEC_HEADS = 2  # Reduced number of decoder heads\n",
    "    ENC_PF_DIM = 64  # Further reduced position-wise feedforward dimension\n",
    "    DEC_PF_DIM = 64  # Further reduced position-wise feedforward dimension\n",
    "    ENC_DROPOUT = 0.1\n",
    "    DEC_DROPOUT = 0.1\n",
    "    \n",
    "    print(f\"Input dimension: {INPUT_DIM}\")\n",
    "    print(f\"Output dimension: {OUTPUT_DIM}\")\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    N_EPOCHS = 5  # Reduced number of epochs\n",
    "    CLIP = 1\n",
    "    LEARNING_RATE = 0.0001\n",
    "    \n",
    "    print(f\"\\nTraining hyperparameters:\")\n",
    "    print(f\"Number of epochs: {N_EPOCHS}\")\n",
    "    print(f\"Gradient clipping: {CLIP}\")\n",
    "    print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "    \n",
    "    # Train for each attention variant\n",
    "    attention_variants = ['multiplicative', 'general', 'additive']\n",
    "    \n",
    "    # Create results table\n",
    "    results_table = {\n",
    "        'Attention Variant': [],\n",
    "        'Training Loss': [],\n",
    "        'Training PPL': [],\n",
    "        'Validation Loss': [],\n",
    "        'Validation PPL': [],\n",
    "        'BLEU Score': [],\n",
    "        'Training Time': []\n",
    "    }\n",
    "    \n",
    "    # Phase 1: Training\n",
    "    print(\"\\n=== Training Phase ===\")\n",
    "    for attn_variant in attention_variants:\n",
    "        print(f\"\\nTraining with {attn_variant} attention...\")\n",
    "        start_training_time = time.time()\n",
    "        \n",
    "        print(\"Initializing encoder and decoder...\")\n",
    "        enc = Encoder(INPUT_DIM, HID_DIM, ENC_LAYERS, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT, attn_variant, device)\n",
    "        dec = Decoder(OUTPUT_DIM, HID_DIM, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, attn_variant, device)\n",
    "        \n",
    "        print(\"Creating Seq2SeqTransformer model...\")\n",
    "        model = Seq2SeqTransformer(enc, dec, PAD_IDX, PAD_IDX, device).to(device)\n",
    "        print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "        \n",
    "        best_valid_loss = float('inf')\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        \n",
    "        print(\"\\nStarting training...\")\n",
    "        for epoch in range(N_EPOCHS):\n",
    "            print(f\"\\nEpoch {epoch+1}/{N_EPOCHS}\")\n",
    "            \n",
    "            print(\"Training...\")\n",
    "            try:\n",
    "                train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
    "            except RuntimeError as e:\n",
    "                if 'out of memory' in str(e):\n",
    "                    print('Out of memory error caught during training. Freeing up memory and retrying...')\n",
    "                    torch.cuda.empty_cache()\n",
    "                    continue\n",
    "                else:\n",
    "                    raise e\n",
    "            \n",
    "            print(\"Evaluating...\")\n",
    "            valid_loss = evaluate(model, valid_loader, criterion)\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            valid_losses.append(valid_loss)\n",
    "            \n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "                print(f\"New best validation loss: {valid_loss:.4f}\")\n",
    "                print(f\"Saving model to en-fr-transformer-{attn_variant}.pt\")\n",
    "                torch.save(model.state_dict(), f'en-fr-transformer-{attn_variant}.pt')\n",
    "            \n",
    "            print(f'Epoch: {epoch+1:02}')\n",
    "            print(f'Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "            print(f'Val. Loss: {valid_loss:.3f} | Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        \n",
    "        # Calculate final metrics\n",
    "        training_time = time.time() - start_training_time\n",
    "        bleu_score = calculate_bleu(model, test_loader, src_tokenizer, trg_tokenizer)\n",
    "        \n",
    "        # Store results\n",
    "        results_table['Attention Variant'].append(attn_variant)\n",
    "        results_table['Training Loss'].append(f\"{train_losses[-1]:.3f}\")\n",
    "        results_table['Training PPL'].append(f\"{math.exp(train_losses[-1]):.3f}\")\n",
    "        results_table['Validation Loss'].append(f\"{valid_losses[-1]:.3f}\")\n",
    "        results_table['Validation PPL'].append(f\"{math.exp(valid_losses[-1]):.3f}\")\n",
    "        results_table['BLEU Score'].append(f\"{bleu_score:.2f}\")\n",
    "        results_table['Training Time'].append(f\"{training_time/60:.1f}m\")\n",
    "        \n",
    "        # Plot training curves\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(valid_losses, label='Valid Loss')\n",
    "        plt.title(f'Training and Validation Losses ({attn_variant} Attention)')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'loss_plot_{attn_variant}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # Print training results table\n",
    "    results_df = pd.DataFrame(results_table)\n",
    "    print(\"\\nTraining Results:\")\n",
    "    print(results_df.to_string(index=False))\n",
    "    results_df.to_csv('attention_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6482ec0-6e77-4a57-8bfa-50643894e6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Attention Visualization ===\n",
      "\n",
      "Model en-de-transformer-multiplicative.pt not found. Skipping.\n",
      "\n",
      "Evaluating general attention model:\n",
      "\n",
      "Generating visualizations...\n",
      "\n",
      "Processing pair:\n",
      "English: The time now is 05:08 .\n",
      "French: The time now is 05:05 .\n",
      "\n",
      "Raw tokens:\n",
      "Source tokens: [2, 4, 96, 149, 11, 0, 172, 3]\n",
      "Target tokens: [2, 1035, 6327, 6869, 2256, 0, 167, 3]\n",
      "\n",
      "Decoded tokens before filtering:\n",
      "Source tokens: ['', 'the', 'time', 'now', 'is', '<unk>', '.', '']\n",
      "Target tokens: ['', 'The', 'time', 'now', 'is', '<unk>', '.', '']\n",
      "\n",
      "Tokens after filtering:\n",
      "Source tokens: ['the', 'time', 'now', 'is', '<unk>', '.']\n",
      "Target tokens: ['The', 'time', 'now', 'is', '<unk>', '.']\n",
      "\n",
      "Attention shape: (7, 8)\n",
      "\n",
      "Saved attention map to: attention_map_general_The_time_now_is_05:0.png\n",
      "\n",
      "Attention Weights:\n",
      "                 The: the(0.08) time(0.39) now(0.11) is(0.10) <unk>(0.13) .(0.03) \n",
      "                time: the(0.16) time(0.07) now(0.14) is(0.09) <unk>(0.16) .(0.02) \n",
      "                 now: the(0.04) time(0.05) now(0.23) is(0.17) <unk>(0.22) .(0.03) \n",
      "                  is: the(0.14) time(0.05) now(0.09) is(0.07) <unk>(0.27) .(0.04) \n",
      "               <unk>: the(0.07) time(0.09) now(0.04) is(0.04) <unk>(0.37) .(0.09) \n",
      "                   .: the(0.15) time(0.07) now(0.05) is(0.06) <unk>(0.26) .(0.07) \n",
      "\n",
      "Processing pair:\n",
      "English: Hello, what's that?\n",
      "French: Qu'est-ce que c'est que ça ?\n",
      "\n",
      "Raw tokens:\n",
      "Source tokens: [2, 2778, 349, 725, 3]\n",
      "Target tokens: [2, 423, 13, 77, 13, 91, 26, 3]\n",
      "\n",
      "Decoded tokens before filtering:\n",
      "Source tokens: ['', 'hello,', \"what's\", 'that?', '']\n",
      "Target tokens: ['', \"Qu'est-ce\", 'que', \"c'est\", 'que', 'ça', '?', '']\n",
      "\n",
      "Tokens after filtering:\n",
      "Source tokens: ['hello,', \"what's\", 'that?']\n",
      "Target tokens: [\"Qu'est-ce\", 'que', \"c'est\", 'que', 'ça', '?']\n",
      "\n",
      "Attention shape: (7, 5)\n",
      "\n",
      "Saved attention map to: attention_map_general_Hello,_what's_that?.png\n",
      "\n",
      "Attention Weights:\n",
      "           Qu'est-ce: hello,(0.07) what's(0.35) that?(0.42) \n",
      "                 que: hello,(0.07) what's(0.15) that?(0.33) \n",
      "               c'est: hello,(0.08) what's(0.16) that?(0.25) \n",
      "                 que: hello,(0.11) what's(0.11) that?(0.23) \n",
      "                  ça: hello,(0.21) what's(0.14) that?(0.13) \n",
      "                   ?: hello,(0.12) what's(0.05) that?(0.13) \n",
      "\n",
      "Testing translations...\n",
      "\n",
      "English: Hello, how are you doing today?\n",
      "French: <unk> tu as dit que tu as raison.\n",
      "\n",
      "English: I love learning new languages.\n",
      "French: Je suis <unk>\n",
      "\n",
      "English: The weather is beautiful today.\n",
      "French: Le <unk> est <unk>\n",
      "\n",
      "English: What time is it?\n",
      "French: Qu'est-ce que tu es <unk>\n",
      "\n",
      "English: Thank you very much.\n",
      "French: Merci.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Evaluation complete! Check the generated visualizations and translation results.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Suppress font and other warnings\n",
    "warnings.filterwarnings(\"ignore\", \"Glyph.*\")\n",
    "warnings.filterwarnings(\"ignore\", \"Matplotlib currently does not support Gujarati natively.*\")\n",
    "\n",
    "# Define constants for special tokens\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "\n",
    "# Define the CustomTokenizer class\n",
    "class CustomTokenizer:\n",
    "    def __init__(self, texts, max_vocab_size=50000, language='en'):\n",
    "        print(f\"\\nInitializing {language} tokenizer...\")\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.language = language\n",
    "        self.word2idx = {'<unk>': UNK_IDX, '<pad>': PAD_IDX, '<sos>': SOS_IDX, '<eos>': EOS_IDX}\n",
    "        self.idx2word = {v: k for k, v in self.word2idx.items()}\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "        \n",
    "        print(f\"Building vocabulary for {language}...\")\n",
    "        word_freq = Counter()\n",
    "        for i, text in enumerate(texts):\n",
    "            if i % 10000 == 0:\n",
    "                print(f\"Processing text {i}/{len(texts)}\")\n",
    "            words = text.lower().split() if language == 'en' else unicodedata.normalize('NFKC', text).split()\n",
    "            word_freq.update(words)\n",
    "        \n",
    "        for word, _ in word_freq.most_common(max_vocab_size - len(self.word2idx)):\n",
    "            self.word2idx[word] = self.vocab_size\n",
    "            self.idx2word[self.vocab_size] = word\n",
    "            self.vocab_size += 1\n",
    "        \n",
    "    def encode(self, text):\n",
    "        words = text.lower().split() if self.language == 'en' else unicodedata.normalize('NFKC', text).split()\n",
    "        return [SOS_IDX] + [self.word2idx.get(word, UNK_IDX) for word in words] + [EOS_IDX]\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        return ' '.join([self.idx2word.get(idx, '<unk>') for idx in indices if idx not in {PAD_IDX, SOS_IDX, EOS_IDX}])\n",
    "\n",
    "# Define the MultiHeadAttentionLayer\n",
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, attn_variant, device):\n",
    "        super().__init__()\n",
    "\n",
    "        assert hid_dim % n_heads == 0\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        self.attn_variant = attn_variant\n",
    "        self.device = device\n",
    "\n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "\n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "\n",
    "        # Split into heads\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        # Calculate attention scores based on variant\n",
    "        energy = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "        attention = self.dropout(attention)\n",
    "\n",
    "        x = torch.matmul(attention, V)\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        x = self.fc_o(x)\n",
    "\n",
    "        return x, attention\n",
    "\n",
    "# Define the EncoderLayer\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, attn_variant, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, attn_variant, device)\n",
    "        self.positionwise_feedforward = nn.Sequential(\n",
    "            nn.Linear(hid_dim, pf_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(pf_dim, hid_dim)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        return src\n",
    "\n",
    "# Define the Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, attn_variant, device, max_length=500):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(hid_dim, n_heads, pf_dim, dropout, attn_variant, device)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        return src\n",
    "\n",
    "# Define the DecoderLayer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, attn_variant, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, attn_variant, device)\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, attn_variant, device)\n",
    "        self.positionwise_feedforward = nn.Sequential(\n",
    "            nn.Linear(hid_dim, pf_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(pf_dim, hid_dim)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        return trg, attention\n",
    "\n",
    "# Define the Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, attn_variant, device, max_length=500):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(hid_dim, n_heads, pf_dim, dropout, attn_variant, device)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        output = self.fc_out(trg)\n",
    "        return output, attention\n",
    "\n",
    "# Define the Seq2SeqTransformer\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=self.device)).bool()\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        return output, attention\n",
    "\n",
    "def translate_sentence(model, sentence, src_tokenizer, trg_tokenizer, device, max_length=128):\n",
    "    \"\"\"\n",
    "    Translate a single English sentence to the target language.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize and encode the source sentence\n",
    "    src_tokens = torch.tensor([src_tokenizer.encode(sentence)]).to(device)\n",
    "\n",
    "    # Initialize target sequence with <sos>\n",
    "    trg_tokens = torch.tensor([[SOS_IDX]]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            with torch.cuda.amp.autocast():\n",
    "                # Get model prediction\n",
    "                output, _ = model(src_tokens, trg_tokens)\n",
    "\n",
    "            # Get the next token prediction\n",
    "            pred_token = output.argmax(2)[:, -1].item()\n",
    "\n",
    "            # Add predicted token to target sequence\n",
    "            trg_tokens = torch.cat([trg_tokens, torch.tensor([[pred_token]]).to(device)], dim=1)\n",
    "\n",
    "            # Stop if <eos> is predicted\n",
    "            if pred_token == EOS_IDX:\n",
    "                break\n",
    "\n",
    "    # Convert tokens back to text\n",
    "    translated_text = trg_tokenizer.decode(trg_tokens.squeeze().cpu().numpy())\n",
    "    return translated_text\n",
    "\n",
    "def evaluate_attention_maps():\n",
    "    \"\"\"\n",
    "    Evaluate attention maps for trained models with detailed debugging\n",
    "    \"\"\"\n",
    "    # Test pair for visualization\n",
    "    test_pairs = [\n",
    "        (\"The time now is 05:08 .\", \"The time now is 05:05 .\"),\n",
    "        (\"Hello, what's that?\", \"Qu'est-ce que c'est que ça ?\")\n",
    "    ]\n",
    "\n",
    "    # Model hyperparameters (must match training)\n",
    "    INPUT_DIM = src_tokenizer.vocab_size\n",
    "    OUTPUT_DIM = trg_tokenizer.vocab_size\n",
    "    HID_DIM = 32\n",
    "    ENC_LAYERS = 1\n",
    "    DEC_LAYERS = 1\n",
    "    ENC_HEADS = 2\n",
    "    DEC_HEADS = 2\n",
    "    ENC_PF_DIM = 64\n",
    "    DEC_PF_DIM = 64\n",
    "    ENC_DROPOUT = 0.1\n",
    "    DEC_DROPOUT = 0.1\n",
    "\n",
    "    print(\"\\n=== Attention Visualization ===\")\n",
    "\n",
    "    for attn_variant, model_path in [('multiplicative', 'en-de-transformer-multiplicative.pt'),\n",
    "                                     ('general', 'en-fr-transformer-general.pt')]:\n",
    "\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"\\nModel {model_path} not found. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nEvaluating {attn_variant} attention model:\")\n",
    "\n",
    "        # Initialize model\n",
    "        enc = Encoder(INPUT_DIM, HID_DIM, ENC_LAYERS, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT, attn_variant, device)\n",
    "        dec = Decoder(OUTPUT_DIM, HID_DIM, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, attn_variant, device)\n",
    "        model = Seq2SeqTransformer(enc, dec, PAD_IDX, PAD_IDX, device).to(device)\n",
    "\n",
    "        # Load model\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        model.eval()\n",
    "\n",
    "        print(\"\\nGenerating visualizations...\")\n",
    "        for src_text, trg_text in test_pairs:\n",
    "            print(f\"\\nProcessing pair:\")\n",
    "            print(f\"English: {src_text}\")\n",
    "            print(f\"French: {trg_text}\")\n",
    "\n",
    "            # Generate attention visualization\n",
    "            with torch.no_grad():\n",
    "                # Tokenize\n",
    "                src_tokens = torch.tensor([src_tokenizer.encode(src_text)]).to(device)\n",
    "                trg_tokens = torch.tensor([trg_tokenizer.encode(trg_text)]).to(device)\n",
    "\n",
    "                # Get model output and attention\n",
    "                output, attention_weights = model(src_tokens, trg_tokens[:,:-1])\n",
    "\n",
    "                # Get last layer attention\n",
    "                if isinstance(attention_weights, list):\n",
    "                    last_layer_attention = attention_weights[-1]\n",
    "                else:\n",
    "                    last_layer_attention = attention_weights\n",
    "\n",
    "                # Get first head's attention from first batch\n",
    "                attention = last_layer_attention[0, 0].cpu().numpy()\n",
    "\n",
    "                # Get tokens\n",
    "                src_tokens_list = src_tokenizer.encode(src_text)\n",
    "                trg_tokens_list = trg_tokenizer.encode(trg_text)\n",
    "\n",
    "                # Print raw tokens for debugging\n",
    "                print(\"\\nRaw tokens:\")\n",
    "                print(\"Source tokens:\", src_tokens_list)\n",
    "                print(\"Target tokens:\", trg_tokens_list)\n",
    "\n",
    "                # Convert token IDs to text\n",
    "                src_tokens_text = [src_tokenizer.decode([token]) for token in src_tokens_list]\n",
    "                trg_tokens_text = [trg_tokenizer.decode([token]) for token in trg_tokens_list]\n",
    "\n",
    "                print(\"\\nDecoded tokens before filtering:\")\n",
    "                print(\"Source tokens:\", src_tokens_text)\n",
    "                print(\"Target tokens:\", trg_tokens_text)\n",
    "\n",
    "                # Remove special tokens\n",
    "                src_tokens_text = [t for t in src_tokens_text if t not in ['<pad>', '<sos>', '<eos>', '']]\n",
    "                trg_tokens_text = [t for t in trg_tokens_text if t not in ['<pad>', '<sos>', '<eos>', '']]\n",
    "\n",
    "                print(\"\\nTokens after filtering:\")\n",
    "                print(\"Source tokens:\", src_tokens_text)\n",
    "                print(\"Target tokens:\", trg_tokens_text)\n",
    "\n",
    "                print(\"\\nAttention shape:\", attention.shape)\n",
    "\n",
    "                # Create visualization\n",
    "                plt.figure(figsize=(12, 8))\n",
    "\n",
    "                # Create heatmap\n",
    "                sns.heatmap(\n",
    "                    attention,  # Use full attention matrix\n",
    "                    xticklabels=src_tokens_text,\n",
    "                    yticklabels=trg_tokens_text,\n",
    "                    cmap='viridis',\n",
    "                    annot=True,\n",
    "                    fmt='.2f',\n",
    "                    square=True,\n",
    "                    cbar_kws={'label': 'Attention Weight'}\n",
    "                )\n",
    "\n",
    "                # Adjust labels\n",
    "                plt.xticks(rotation=45, ha='right')\n",
    "                plt.yticks(rotation=0)\n",
    "\n",
    "                plt.title(f'Attention Weights Visualization\\n{attn_variant.capitalize()} Attention', pad=20)\n",
    "                plt.xlabel('Source Text (English)', labelpad=10)\n",
    "                plt.ylabel('Target Text (French)', labelpad=10)\n",
    "\n",
    "                plt.tight_layout()\n",
    "\n",
    "                # Save plot\n",
    "                filename = f'attention_map_{attn_variant}_{src_text[:20].replace(\" \", \"_\")}.png'\n",
    "                plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "\n",
    "                print(f\"\\nSaved attention map to: {filename}\")\n",
    "\n",
    "                # Print attention weights\n",
    "                print(\"\\nAttention Weights:\")\n",
    "                for i in range(min(len(trg_tokens_text), attention.shape[0])):\n",
    "                    print(f\"{trg_tokens_text[i]:>20}: \", end=\"\")\n",
    "                    for j in range(min(len(src_tokens_text), attention.shape[1])):\n",
    "                        print(f\"{src_tokens_text[j]}({attention[i,j]:.2f}) \", end=\"\")\n",
    "                    print()\n",
    "\n",
    "        print(\"\\nTesting translations...\")\n",
    "        test_sentences = [\n",
    "            \"Hello, how are you doing today?\",\n",
    "            \"I love learning new languages.\",\n",
    "            \"The weather is beautiful today.\",\n",
    "            \"What time is it?\",\n",
    "            \"Thank you very much.\"\n",
    "        ]\n",
    "\n",
    "        for text in test_sentences:\n",
    "            translated = translate_sentence(model, text, src_tokenizer, trg_tokenizer, device, max_length=50)\n",
    "            print(f\"\\nEnglish: {text}\")\n",
    "            print(f\"French: {translated}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "    print(\"\\nEvaluation complete! Check the generated visualizations and translation results.\")\n",
    "\n",
    "# Run the evaluation\n",
    "evaluate_attention_maps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189f34ff-7e5c-40fc-83a5-8c59f3168ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0152f2-3563-4e78-aeb9-3880058ec05a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b7e7c2-242a-4d0c-b20d-d1b37b602226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
