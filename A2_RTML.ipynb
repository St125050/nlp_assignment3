{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89756ec5-a3b7-403c-bbba-d2d1042b47bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-29 17:31:07--  https://github.com/AlexeyAB/darknet/blob/master/cfg/yolov4.cfg\n",
      "Resolving github.com (github.com)... 20.205.243.166\n",
      "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘yolov4.cfg’\n",
      "\n",
      "yolov4.cfg              [ <=>                ] 622.78K  3.62MB/s    in 0.2s    \n",
      "\n",
      "2025-01-29 17:31:08 (3.62 MB/s) - ‘yolov4.cfg’ saved [637724]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## download the yolov4 cfg file\n",
    "# !mkdir -p cfg\n",
    "!wget https://github.com/AlexeyAB/darknet/blob/master/cfg/yolov4.cfg\n",
    "!mv yolov4.cfg cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "548f65d1-46d7-4c2f-a19b-98eafc871112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-29 17:31:14--  https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/darknet.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 11533 (11K) [text/plain]\n",
      "Saving to: ‘darknet.py’\n",
      "\n",
      "darknet.py          100%[===================>]  11.26K  --.-KB/s    in 0.002s  \n",
      "\n",
      "2025-01-29 17:31:15 (6.88 MB/s) - ‘darknet.py’ saved [11533/11533]\n",
      "\n",
      "--2025-01-29 17:31:15--  https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/util.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7432 (7.3K) [text/plain]\n",
      "Saving to: ‘util.py’\n",
      "\n",
      "util.py             100%[===================>]   7.26K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-01-29 17:31:16 (45.6 MB/s) - ‘util.py’ saved [7432/7432]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/darknet.py\n",
    "!wget https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/util.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "921a112c-e44f-4df5-8878-5ab5e4da044f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## parse the yolov4 module\n",
    "#!pip install opencv-python\n",
    "import darknet\n",
    "\n",
    "blocks = darknet.parse_cfg(\"cfg/yolov4.cfg\")\n",
    "net_info, module_list = darknet.create_modules(blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37dd482c-ea03-4ae8-a6fc-a57d15cb4d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "\n",
    "class Darknet(nn.Module):\n",
    "    def __init__(self, blocks, module_list,net_info, CUDA):\n",
    "        super(Darknet, self).__init__()\n",
    "        self.blocks = blocks\n",
    "        self.net_info = net_info\n",
    "        self.module_list = module_list\n",
    "        self.CUDA = CUDA\n",
    "        \n",
    "    def forward(self, x):\n",
    "        modules = self.blocks[1:]\n",
    "        outputs = {}   #We cache the outputs for the route layer\n",
    "        \n",
    "        write = 0\n",
    "        for i, module in enumerate(modules):        \n",
    "            module_type = (module[\"type\"])\n",
    "            \n",
    "            if module_type == \"convolutional\" or module_type == \"upsample\":\n",
    "                # print(i, module_type, self.module_list[i])\n",
    "                x = self.module_list[i](x)\n",
    "    \n",
    "            elif module_type == \"route\":\n",
    "                layers = module[\"layers\"]\n",
    "                layers = [int(a) for a in layers]\n",
    "                if len(layers) == 1:\n",
    "                    # Output feature maps from the layer indexed by the value\n",
    "                    if layers[0] < 0:\n",
    "                        connect_layer = i + layers[0]\n",
    "                        x = outputs[connect_layer]\n",
    "                    elif layers[0] > 0:\n",
    "                        connect_layer = layers[0]\n",
    "                        x = outputs[connect_layer]\n",
    "                        \n",
    "                elif len(layers) == 2:\n",
    "                    # Concatenate feature maps from two layers\n",
    "                    connect_layer1 = i + (layers[0])\n",
    "                    connect_layer2 = i+ (layers[1])\n",
    "                    x = torch.cat((outputs[connect_layer1], outputs[connect_layer2]), dim=1)\n",
    "                elif len(layers) == 4:\n",
    "                    connect_layer1 = i + (layers[0])\n",
    "                    connect_layer2 = i + (layers[1])\n",
    "                    connect_layer3 = i + (layers[2])\n",
    "                    connect_layer4 = i + (layers[3])\n",
    "                    x = torch.cat((outputs[connect_layer1], outputs[connect_layer2], outputs[connect_layer3], outputs[connect_layer3]), dim=1)\n",
    "\n",
    "            elif  module_type == \"shortcut\":\n",
    "                from_ = int(module[\"from\"])\n",
    "                x = outputs[i-1] + outputs[i+from_]\n",
    "    \n",
    "            elif module_type == 'yolo':    \n",
    "                anchors = self.module_list[i][0].anchors\n",
    "                #Get the input dimensions\n",
    "                print(self.net_info[\"height\"])\n",
    "                inp_dim = int(self.net_info[\"height\"])\n",
    "        \n",
    "                #Get the number of classes\n",
    "                num_classes = int(module[\"classes\"])\n",
    "        \n",
    "                #Transform \n",
    "                x = x.data\n",
    "                x = predict_transform(x, inp_dim, anchors, num_classes, CUDA=False)\n",
    "                if not write:              #if no collector has been intialised. \n",
    "                    detections = x\n",
    "                    write = 1\n",
    "        \n",
    "                else:       \n",
    "                    detections = torch.cat((detections, x), 1)\n",
    "        \n",
    "            outputs[i] = x\n",
    "        \n",
    "        return detections\n",
    "\n",
    "\n",
    "    def load_weights(self, weightfile):\n",
    "        #Open the weights file\n",
    "        fp = open(weightfile, \"rb\")\n",
    "    \n",
    "        #The first 5 values are header information \n",
    "        # 1. Major version number\n",
    "        # 2. Minor Version Number\n",
    "        # 3. Subversion number \n",
    "        # 4,5. Images seen by the network (during training)\n",
    "        header = np.fromfile(fp, dtype = np.int32, count = 5)\n",
    "        self.header = torch.from_numpy(header)\n",
    "        self.seen = self.header[3]   \n",
    "        \n",
    "        weights = np.fromfile(fp, dtype = np.float32)\n",
    "        \n",
    "        ptr = 0\n",
    "        for i in range(len(self.module_list)):\n",
    "            module_type = self.blocks[i + 1][\"type\"]\n",
    "    \n",
    "            #If module_type is convolutional load weights\n",
    "            #Otherwise ignore.\n",
    "            \n",
    "            if module_type == \"convolutional\":\n",
    "                model = self.module_list[i]\n",
    "                try:\n",
    "                    batch_normalize = int(self.blocks[i+1][\"batch_normalize\"])\n",
    "                except:\n",
    "                    batch_normalize = 0\n",
    "            \n",
    "                conv = model[0]\n",
    "                \n",
    "                \n",
    "                if (batch_normalize):\n",
    "                    bn = model[1]\n",
    "        \n",
    "                    #Get the number of weights of Batch Norm Layer\n",
    "                    num_bn_biases = bn.bias.numel()\n",
    "        \n",
    "                    #Load the weights\n",
    "                    bn_biases = torch.from_numpy(weights[ptr:ptr + num_bn_biases])\n",
    "                    ptr += num_bn_biases\n",
    "        \n",
    "                    bn_weights = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
    "                    ptr  += num_bn_biases\n",
    "        \n",
    "                    bn_running_mean = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
    "                    ptr  += num_bn_biases\n",
    "        \n",
    "                    bn_running_var = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
    "                    ptr  += num_bn_biases\n",
    "        \n",
    "                    #Cast the loaded weights into dims of model weights. \n",
    "                    bn_biases = bn_biases.view_as(bn.bias.data)\n",
    "                    bn_weights = bn_weights.view_as(bn.weight.data)\n",
    "                    bn_running_mean = bn_running_mean.view_as(bn.running_mean)\n",
    "                    bn_running_var = bn_running_var.view_as(bn.running_var)\n",
    "        \n",
    "                    #Copy the data to model\n",
    "                    bn.bias.data.copy_(bn_biases)\n",
    "                    bn.weight.data.copy_(bn_weights)\n",
    "                    bn.running_mean.copy_(bn_running_mean)\n",
    "                    bn.running_var.copy_(bn_running_var)\n",
    "                \n",
    "                else:\n",
    "                    #Number of biases\n",
    "                    num_biases = conv.bias.numel()\n",
    "                \n",
    "                    #Load the weights\n",
    "                    conv_biases = torch.from_numpy(weights[ptr: ptr + num_biases])\n",
    "                    ptr = ptr + num_biases\n",
    "                \n",
    "                    #reshape the loaded weights according to the dims of the model weights\n",
    "                    conv_biases = conv_biases.view_as(conv.bias.data)\n",
    "                \n",
    "                    #Finally copy the data\n",
    "                    conv.bias.data.copy_(conv_biases)\n",
    "                    \n",
    "                #Let us load the weights for the Convolutional layers\n",
    "                num_weights = conv.weight.numel()\n",
    "                \n",
    "                #Do the same as above for weights\n",
    "                conv_weights = torch.from_numpy(weights[ptr:ptr+num_weights])\n",
    "                ptr = ptr + num_weights\n",
    "                \n",
    "                conv_weights = conv_weights.view_as(conv.weight.data)\n",
    "                conv.weight.data.copy_(conv_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a345ee91-ee05-4510-a6bf-990a237d8321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading weights fro darknet \n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "yolo_v4=Darknet(blocks, module_list, net_info, CUDA='cpu').to('cpu')\n",
    "yolo_v4.load_weights('yolov4.weights')\n",
    "# load_weights_into_model(yolo_v4,'yolov4.weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5474c0a-acd4-47cc-bc66-0f75c0bca95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary of the yolov4 with dense connections\n",
    "# summary(yolo_v4.to(device), (3,608,608)).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bdb378cc-da88-4a8d-9e1a-cd63f3833f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-30 11:41:39--  https://github.com/ayooshkathuria/pytorch-yolo-v3/raw/master/dog-cycle-car.png\n",
      "Resolving github.com (github.com)... 20.205.243.166\n",
      "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/ayooshkathuria/pytorch-yolo-v3/master/dog-cycle-car.png [following]\n",
      "--2025-01-30 11:41:39--  https://raw.githubusercontent.com/ayooshkathuria/pytorch-yolo-v3/master/dog-cycle-car.png\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
      "connected. to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... \n",
      "200 OKequest sent, awaiting response... \n",
      "Length: 347445 (339K) [image/png]\n",
      "Saving to: ‘dog-cycle-car.png’\n",
      "\n",
      "dog-cycle-car.png   100%[===================>] 339.30K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2025-01-30 11:41:41 (3.43 MB/s) - ‘dog-cycle-car.png’ saved [347445/347445]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/ayooshkathuria/pytorch-yolo-v3/raw/master/dog-cycle-car.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "950efe14-1c84-4f32-a7cc-040c570d3060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def normalize_tensor_image(tensor_image):\n",
    "    \"\"\"\n",
    "    Normalize the input tensor image using ImageNet mean and std values.\n",
    "    The image tensor is assumed to be in the format [C, H, W] (channels, height, width).\n",
    "    \"\"\"\n",
    "    # Define mean and standard deviation values for ImageNet (you can adjust for other datasets)\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32).view(3, 1, 1)  # [C, 1, 1]\n",
    "    std = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32).view(3, 1, 1)   # [C, 1, 1]\n",
    "\n",
    "    # Normalize the image tensor\n",
    "    normalized_tensor = (tensor_image / 255.0 - mean) / std  # First scale, then subtract mean, divide by std\n",
    "    return normalized_tensor\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `img_tensor` is the input tensor with shape [C, H, W]\n",
    "img_tensor = torch.randn(3, 224, 224)  # Example tensor (replace with your actual image tensor)\n",
    "\n",
    "normalized_img = normalize_tensor_image(img_tensor)\n",
    "print(normalized_img.shape)  # It should be the same shape as input [C, H, W]\n",
    "\n",
    "\n",
    "def get_test_input():\n",
    "    img = cv2.imread(\"dog-cycle-car.png\")\n",
    "    img = cv2.resize(img, (608,608))          #Resize to the input dimension\n",
    "    img_ =  img[:,:,::-1].transpose((2,0,1))  # BGR -> RGB | H X W C -> C X H X W \n",
    "    img_ = img_[np.newaxis,:,:,:]/255.0       #Add a channel at 0 (for batch) | Normalise\n",
    "    img_ = torch.from_numpy(img_).float()     #Convert to float\n",
    "    img_ = Variable(img_)                     # Convert to Variable\n",
    "    return img_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f6ed325-d0b8-44b1-be5c-f734998f8f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "608\n",
      "608\n",
      "608\n"
     ]
    }
   ],
   "source": [
    "from util import *\n",
    "\n",
    "#model = MyDarknet(\"cfg/yolov3.cfg\")\n",
    "inp = get_test_input()\n",
    "inp=normalize_tensor_image(inp)\n",
    "pred_ = yolo_v4(inp)\n",
    "# print (pred_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebe18268-d6d2-49cd-8413-70158377c17e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0937e+00, 1.8076e+00, 1.7396e+01,  ..., 5.0362e-01,\n",
       "          8.4453e-01, 8.4273e-02],\n",
       "         [1.2765e+00, 6.3729e-01, 5.4941e+01,  ..., 3.7434e-01,\n",
       "          2.2933e-01, 3.7951e-01],\n",
       "         [5.4570e-01, 1.0080e+00, 1.0801e+01,  ..., 2.2132e-01,\n",
       "          7.0085e-01, 2.2979e-01],\n",
       "         ...,\n",
       "         [6.0001e+02, 5.9996e+02, 1.4375e+02,  ..., 4.9847e-01,\n",
       "          4.9989e-01, 4.9411e-01],\n",
       "         [5.9992e+02, 6.0000e+02, 1.8958e+02,  ..., 5.0153e-01,\n",
       "          4.9924e-01, 4.9375e-01],\n",
       "         [5.9995e+02, 5.9996e+02, 4.5438e+02,  ..., 4.9870e-01,\n",
       "          4.9830e-01, 4.9466e-01]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1deca2c6-1d33-4808-99ad-3ee1590b18ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp=get_test_input()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dfb69ff-ab50-4d08-804b-35847039bedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16142, 8])\n"
     ]
    }
   ],
   "source": [
    "#using non_maximum_supression\n",
    "out=write_results(pred_.detach(), 0.5, 80, nms_conf = 0.4)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53e3e33f-d8dd-4b36-92a6-087790c1ed1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No predictions found for class person\n",
      "Max probability for class bicycle: 0.5413703918457031\n",
      "Max probability for class car: 0.6058207750320435\n",
      "Max probability for class motorbike: 0.5304465889930725\n",
      "No predictions found for class aeroplane\n",
      "Max probability for class bus: 0.517562210559845\n",
      "No predictions found for class train\n",
      "No predictions found for class truck\n",
      "Max probability for class boat: 0.5085288286209106\n",
      "Max probability for class traffic light: 0.5562366247177124\n",
      "Max probability for class fire hydrant: 0.580032467842102\n",
      "No predictions found for class stop sign\n",
      "No predictions found for class parking meter\n",
      "Max probability for class bench: 0.5349060297012329\n",
      "No predictions found for class bird\n",
      "Max probability for class cat: 0.5429936647415161\n",
      "Max probability for class dog: 0.8789980411529541\n",
      "Max probability for class horse: 0.5355207920074463\n",
      "Max probability for class sheep: 0.5327343940734863\n",
      "Max probability for class cow: 0.5053259134292603\n",
      "Max probability for class elephant: 0.5187659859657288\n",
      "Max probability for class bear: 0.5367901921272278\n",
      "No predictions found for class zebra\n",
      "Max probability for class giraffe: 0.5771916508674622\n",
      "Max probability for class backpack: 0.7455521821975708\n",
      "Max probability for class umbrella: 0.5427785515785217\n",
      "Max probability for class handbag: 0.8651899099349976\n",
      "Max probability for class tie: 0.7685981392860413\n",
      "No predictions found for class suitcase\n",
      "No predictions found for class frisbee\n",
      "No predictions found for class skis\n",
      "Max probability for class snowboard: 0.8739625215530396\n",
      "Max probability for class sports ball: 0.9134529232978821\n",
      "Max probability for class kite: 0.6756226420402527\n",
      "Max probability for class baseball bat: 0.6189736127853394\n",
      "Max probability for class baseball glove: 0.7287386059761047\n",
      "Max probability for class skateboard: 0.5347818732261658\n",
      "Max probability for class surfboard: 0.5289948582649231\n",
      "Max probability for class tennis racket: 0.5948018431663513\n",
      "No predictions found for class bottle\n",
      "No predictions found for class wine glass\n",
      "No predictions found for class cup\n",
      "Max probability for class fork: 0.5293952226638794\n",
      "Max probability for class knife: 0.7903503775596619\n",
      "Max probability for class spoon: 0.8595031499862671\n",
      "Max probability for class bowl: 0.5574565529823303\n",
      "Max probability for class banana: 0.843610405921936\n",
      "Max probability for class apple: 0.5288521647453308\n",
      "No predictions found for class sandwich\n",
      "No predictions found for class orange\n",
      "No predictions found for class broccoli\n",
      "No predictions found for class carrot\n",
      "Max probability for class hot dog: 0.7412147521972656\n",
      "No predictions found for class pizza\n",
      "Max probability for class donut: 0.5862631797790527\n",
      "Max probability for class cake: 0.6910131573677063\n",
      "No predictions found for class chair\n",
      "Max probability for class sofa: 0.5562990307807922\n",
      "Max probability for class pottedplant: 0.5310468077659607\n",
      "No predictions found for class bed\n",
      "Max probability for class diningtable: 0.5820738077163696\n",
      "Max probability for class toilet: 0.9382035136222839\n",
      "Max probability for class tvmonitor: 0.5341780781745911\n",
      "Max probability for class laptop: 0.7981027960777283\n",
      "No predictions found for class mouse\n",
      "Max probability for class remote: 0.5077677369117737\n",
      "Max probability for class keyboard: 0.7289772629737854\n",
      "Max probability for class cell phone: 0.540324866771698\n",
      "Max probability for class microwave: 0.6289253830909729\n",
      "No predictions found for class oven\n",
      "Max probability for class toaster: 0.5135583877563477\n",
      "Max probability for class sink: 0.5032694935798645\n",
      "No predictions found for class refrigerator\n",
      "Max probability for class book: 0.546251654624939\n",
      "No predictions found for class clock\n",
      "No predictions found for class vase\n",
      "No predictions found for class scissors\n",
      "Max probability for class teddy bear: 0.5410533547401428\n",
      "Max probability for class hair drier: 0.5760967135429382\n",
      "Max probability for class toothbrush: 0.5273802280426025\n"
     ]
    }
   ],
   "source": [
    "def load_classes(namesfile):\n",
    "    fp = open(namesfile, \"r\")\n",
    "    names = fp.read().split(\"\\n\")[:-1]\n",
    "    return names\n",
    "\n",
    "num_classes = 80\n",
    "classes = load_classes(\"data/coco.names\")\n",
    "idx = []\n",
    "for i in range(num_classes):\n",
    "    # Filter predictions for the current class\n",
    "    class_preds = out[out[:, -1] == i]  # Filter based on class ID (last column)\n",
    "    \n",
    "    if class_preds.size(0) > 0:  # Ensure there are predictions for the class\n",
    "        max_prob = torch.max(class_preds[:, 5])\n",
    "        print(f\"Max probability for class {classes[i]}: {max_prob.item()}\")\n",
    "    else:\n",
    "        print(f\"No predictions found for class {classes[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bf90aaa-263f-4b65-ac9e-58d536ad3c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No predictions found for class person\n",
      "No predictions found for class bicycle\n",
      "No predictions found for class car\n",
      "No predictions found for class motorbike\n",
      "No predictions found for class aeroplane\n",
      "No predictions found for class bus\n",
      "No predictions found for class train\n",
      "No predictions found for class truck\n",
      "No predictions found for class boat\n",
      "No predictions found for class traffic light\n",
      "No predictions found for class fire hydrant\n",
      "No predictions found for class stop sign\n",
      "No predictions found for class parking meter\n",
      "No predictions found for class bench\n",
      "No predictions found for class bird\n",
      "No predictions found for class cat\n",
      "No predictions found for class dog\n",
      "No predictions found for class horse\n",
      "No predictions found for class sheep\n",
      "No predictions found for class cow\n",
      "No predictions found for class elephant\n",
      "No predictions found for class bear\n",
      "No predictions found for class zebra\n",
      "No predictions found for class giraffe\n",
      "No predictions found for class backpack\n",
      "No predictions found for class umbrella\n",
      "No predictions found for class handbag\n",
      "No predictions found for class tie\n",
      "No predictions found for class suitcase\n",
      "No predictions found for class frisbee\n",
      "No predictions found for class skis\n",
      "No predictions found for class snowboard\n",
      "No predictions found for class kite\n",
      "No predictions found for class baseball bat\n",
      "No predictions found for class baseball glove\n",
      "No predictions found for class skateboard\n",
      "No predictions found for class surfboard\n",
      "No predictions found for class tennis racket\n",
      "No predictions found for class bottle\n",
      "No predictions found for class wine glass\n",
      "No predictions found for class cup\n",
      "No predictions found for class fork\n",
      "No predictions found for class knife\n",
      "No predictions found for class spoon\n",
      "No predictions found for class bowl\n",
      "No predictions found for class banana\n",
      "No predictions found for class apple\n",
      "No predictions found for class sandwich\n",
      "No predictions found for class orange\n",
      "No predictions found for class broccoli\n",
      "No predictions found for class carrot\n",
      "No predictions found for class hot dog\n",
      "No predictions found for class pizza\n",
      "No predictions found for class donut\n",
      "No predictions found for class cake\n",
      "No predictions found for class chair\n",
      "No predictions found for class sofa\n",
      "No predictions found for class pottedplant\n",
      "No predictions found for class bed\n",
      "No predictions found for class diningtable\n",
      "No predictions found for class tvmonitor\n",
      "No predictions found for class laptop\n",
      "No predictions found for class mouse\n",
      "No predictions found for class remote\n",
      "No predictions found for class keyboard\n",
      "No predictions found for class cell phone\n",
      "No predictions found for class microwave\n",
      "No predictions found for class oven\n",
      "No predictions found for class toaster\n",
      "No predictions found for class sink\n",
      "No predictions found for class refrigerator\n",
      "No predictions found for class book\n",
      "No predictions found for class clock\n",
      "No predictions found for class vase\n",
      "No predictions found for class scissors\n",
      "No predictions found for class teddy bear\n",
      "No predictions found for class hair drier\n",
      "No predictions found for class toothbrush\n",
      "Image saved as 'output_image_with_bboxes.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@73.653] global loadsave.cpp:848 imwrite_ Unsupported depth image for selected encoder is fallbacked to CV_8U.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import random\n",
    "\n",
    "def load_classes(namesfile):\n",
    "    with open(namesfile, \"r\") as fp:\n",
    "        names = fp.read().split(\"\\n\")[:-1]\n",
    "    return names\n",
    "\n",
    "# Load COCO class names\n",
    "classes = load_classes(\"data/coco.names\")\n",
    "\n",
    "# Assuming 'out' is the predictions tensor, shape = [num_predictions, 8]\n",
    "# Columns: [image_index, x_min, y_min, x_max, y_max, confidence, class_probabilities, class_id]\n",
    "\n",
    "# Set up the colors for drawing bounding boxes\n",
    "colors = [[255, 0, 0], [255, 0, 0], [255, 255, 0], [0, 255, 0], [0, 255, 255], [0, 0, 255], [255, 0, 255]]\n",
    "\n",
    "# Sample image for demonstration, replace with actual image loading\n",
    "img = inp[0].permute(1, 2, 0).numpy()  # Convert from [C, H, W] to [H, W, C]\n",
    "# Filter out predictions with low confidence\n",
    "confidence_threshold = 0.9\n",
    "filtered_preds = out[out[:, 5] > confidence_threshold]\n",
    "# Iterate over each class\n",
    "for i in range(len(classes)):\n",
    "    # Filter predictions for the current class (assuming class_id is in the last column, index 7)\n",
    "    # class_preds = out[out[:, -1] == i]  # Filter based on class ID (last column)\n",
    "    class_preds = filtered_preds[filtered_preds[:, -1] == i]  # class_id is in column index -1\n",
    "    if class_preds.size(0) > 0:  # Ensure there are predictions for the class\n",
    "        # Find the index of the prediction with the maximum confidence\n",
    "        max_idx = torch.argmax(class_preds[:, 5])  # Confidence is assumed to be in the 6th column (index 5)\n",
    "        \n",
    "        # Extract the bounding box coordinates and confidence for the max probability prediction\n",
    "        bbox = class_preds[max_idx]  # Get the prediction with the max confidence\n",
    "        x_min, y_min, x_max, y_max = bbox[1:5]  # Extract the bounding box coordinates\n",
    "        \n",
    "        # Convert the bounding box coordinates to integers (for drawing)\n",
    "        c1 = (int(x_min), int(y_min))  # Top-left corner (x_min, y_min)\n",
    "        c2 = (int(x_max), int(y_max))  # Bottom-right corner (x_max, y_max)\n",
    "        \n",
    "        # Pick a random color for the bounding box\n",
    "        color = random.choice(colors)\n",
    "        \n",
    "        # Draw the rectangle on the image\n",
    "        cv2.rectangle(img, c1, c2, color, 2)  # Draw rectangle with thickness 2\n",
    "        \n",
    "        # Get the class label and confidence score\n",
    "        label = classes[i]  # Get class label from the classes list\n",
    "        confidence = bbox[5].item()  # Confidence is in the 6th column (index 5)\n",
    "        \n",
    "        # Create label text\n",
    "        label_text = f\"{label}: {confidence:.2f}\"\n",
    "        \n",
    "        # Draw the label background\n",
    "        t_size = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_PLAIN, 1, 1)[0]\n",
    "        c2 = (c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4)\n",
    "        cv2.rectangle(img, c1, c2, color, -1)  # Draw filled rectangle for label background\n",
    "        \n",
    "        # Put the label text on the image\n",
    "        cv2.putText(img, label_text, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225, 255, 255], 1)\n",
    "    \n",
    "    else:\n",
    "        print(f\"No predictions found for class {classes[i]}\")\n",
    "\n",
    "# Save the image with bounding boxes to a file\n",
    "cv2.imwrite('output_image_with_bboxes.jpg', img)  # Save the result as an image\n",
    "\n",
    "print(\"Image saved as 'output_image_with_bboxes.jpg'\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9da671d6-f77f-4697-a632-84285f792746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[255.0000, 255.0000, 255.0000,  ...,  -2.1066,  -2.1131,  -2.1135],\n",
       "          [255.0000, 255.0000, 255.0000,  ..., 255.0000, 255.0000, 255.0000],\n",
       "          [255.0000, 255.0000, 255.0000,  ..., 255.0000, 255.0000, 255.0000],\n",
       "          ...,\n",
       "          [255.0000, 255.0000,  -2.1072,  ...,  -2.1109,  -2.1137,  -2.1143],\n",
       "          [255.0000, 255.0000,  -2.1072,  ..., 255.0000, 255.0000, 255.0000],\n",
       "          [255.0000, 255.0000,  -2.1072,  ..., 255.0000, 255.0000, 255.0000]],\n",
       "\n",
       "         [[  0.0000,   0.0000,   0.0000,  ...,  -2.0236,  -2.0306,  -2.0312],\n",
       "          [  0.0000,   0.0000,   0.0000,  ..., 255.0000, 255.0000, 255.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,  ..., 255.0000, 255.0000, 255.0000],\n",
       "          ...,\n",
       "          [  0.0000,   0.0000,  -2.0241,  ...,  -2.0288,  -2.0317,  -2.0324],\n",
       "          [  0.0000,   0.0000,  -2.0242,  ..., 255.0000, 255.0000, 255.0000],\n",
       "          [  0.0000,   0.0000,  -2.0242,  ..., 255.0000, 255.0000, 255.0000]],\n",
       "\n",
       "         [[  0.0000,   0.0000,   0.0000,  ...,  -1.7969,  -1.8025,  -1.8025],\n",
       "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "          ...,\n",
       "          [  0.0000,   0.0000,  -1.7920,  ...,  -1.7978,  -1.8007,  -1.8013],\n",
       "          [  0.0000,   0.0000,  -1.7921,  ...,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,  -1.7921,  ...,   0.0000,   0.0000,   0.0000]]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c01d89d-de0c-4a4a-a86b-fb067b5d0816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17949, 8])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40018a80-a003-46b9-b668-28361c7342f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-30 15:05:03--  https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/data/coco.names\n",
      "185.199.110.133, 185.199.111.133, 185.199.109.133, ...tent.com)... \n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 625 [text/plain]\n",
      "Saving to: ‘coco.names’\n",
      "\n",
      "coco.names          100%[===================>]     625  --.-KB/s    in 0s      \n",
      "\n",
      "2025-01-30 15:05:04 (26.0 MB/s) - ‘coco.names’ saved [625/625]\n",
      "\n",
      "mkdir: cannot create directory ‘data’: File exists\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/data/coco.names\n",
    "!mkdir data\n",
    "!mv coco.names data/coco.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fce4d77-0517-4310-adc9-f36d95d13598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-30 15:05:39--  https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/detect.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
      "connected. to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... \n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7273 (7.1K) [text/plain]\n",
      "Saving to: ‘detect.py’\n",
      "\n",
      "detect.py           100%[===================>]   7.10K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-01-30 15:05:40 (44.4 MB/s) - ‘detect.py’ saved [7273/7273]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/detect.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6635102-e806-46b1-9816-27721a8de302",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p cocoimages\n",
    "!cp dog-cycle-car.png cocoimages/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bb5b0cf-8727-4cce-91fc-17473289febd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (608, 608, 3)\n",
      "(608, 608, 3)\n",
      "(608, 608, 3)\n",
      "tensor([[608., 608., 608., 608.]])\n",
      "608\n",
      "608\n",
      "608\n",
      "dog-cycle-car.png    predicted in 70.530 seconds\n",
      "----------------------------------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 191\u001b[0m\n\u001b[1;32m    186\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mputText(img, label, (c1[\u001b[38;5;241m0\u001b[39m], c1[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m t_size[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m4\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_PLAIN, \u001b[38;5;241m1\u001b[39m, [\u001b[38;5;241m225\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n\u001b[0;32m--> 191\u001b[0m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaded_ims\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m det_names \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(imlist)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/det_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdes\u001b[39m\u001b[38;5;124m\"\u001b[39m,x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(cv2\u001b[38;5;241m.\u001b[39mimwrite, det_names, [cv2\u001b[38;5;241m.\u001b[39mcvtColor(loaded_ims[\u001b[38;5;241m0\u001b[39m], cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)]))\n",
      "Cell \u001b[0;32mIn[11], line 191\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    186\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mputText(img, label, (c1[\u001b[38;5;241m0\u001b[39m], c1[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m t_size[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m4\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_PLAIN, \u001b[38;5;241m1\u001b[39m, [\u001b[38;5;241m225\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n\u001b[0;32m--> 191\u001b[0m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaded_ims\u001b[49m\u001b[43m)\u001b[49m, output))\n\u001b[1;32m    193\u001b[0m det_names \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(imlist)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/det_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdes\u001b[39m\u001b[38;5;124m\"\u001b[39m,x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(cv2\u001b[38;5;241m.\u001b[39mimwrite, det_names, [cv2\u001b[38;5;241m.\u001b[39mcvtColor(loaded_ims[\u001b[38;5;241m0\u001b[39m], cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)]))\n",
      "Cell \u001b[0;32mIn[11], line 156\u001b[0m, in \u001b[0;36mwrite\u001b[0;34m(x, results)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite\u001b[39m(x, results):\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Extract coordinates and class label from YOLOv4 output\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m     c1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Top-left corner (x_min, y_min)\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     c2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, x[\u001b[38;5;241m3\u001b[39m:\u001b[38;5;241m5\u001b[39m]))  \u001b[38;5;66;03m# Bottom-right corner (x_max, y_max)\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     img \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;28mint\u001b[39m(x[\u001b[38;5;241m0\u001b[39m])]  \u001b[38;5;66;03m# Get the corresponding image from results list\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import time\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import cv2 \n",
    "from util import *\n",
    "import argparse\n",
    "import os \n",
    "import os.path as osp\n",
    "# from darknet import Darknet\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "images = \"cocoimages\"\n",
    "batch_size = 4\n",
    "confidence = 0.5\n",
    "nms_thesh = 0.2\n",
    "start = 0\n",
    "CUDA = False\n",
    "\n",
    "num_classes = 80\n",
    "classes = load_classes(\"data/coco.names\")\n",
    "\n",
    "#Set up the neural network\n",
    "# model=Darknet(blocks, module_list, net_info, CUDA='cpu').to('cpu')\n",
    "# model.load_weights('yolov4.weights')\n",
    "\n",
    "yolo_v4.net_info[\"height\"] = 608\n",
    "yolo_v4.net_info[\"width\"] = 608\n",
    "inp_dim = int(yolo_v4.net_info[\"height\"])\n",
    "yolo_v4.eval()\n",
    "read_dir = time.time()\n",
    "assert inp_dim % 32 == 0 \n",
    "assert inp_dim > 32\n",
    "try:\n",
    "    imlist = [osp.join(osp.realpath('.'), images, img) for img in os.listdir(images)]\n",
    "except NotADirectoryError:\n",
    "    imlist = []\n",
    "    imlist.append(osp.join(osp.realpath('.'), images))\n",
    "except FileNotFoundError:\n",
    "    print (\"No file or directory with the name {}\".format(images))\n",
    "    exit()\n",
    "    \n",
    "if not os.path.exists(\"des\"):\n",
    "    os.makedirs(\"des\")\n",
    "\n",
    "load_batch = time.time()\n",
    "# loaded_ims = [letterbox_image(cv2.imread(x), (inp_dim, inp_dim)) for x in imlist]\n",
    "\n",
    "# img = cv2.imread(imlist[0])\n",
    "\n",
    "print(type(img), img.shape)\n",
    "img = letterbox_image(img, (inp_dim, inp_dim))\n",
    "cv2.imwrite('test.jpg', img)\n",
    "img = cv2.imread('test.jpg')\n",
    "print(img.shape)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "# img = torch.from_numpy(img).float().div(255.0).unsqueeze(0)\n",
    "# print(img.shape)\n",
    "# img = torch.from_numpy(img.transpose(2, 0, 1)).float().div(255.0).unsqueeze(0)\n",
    "print(img.shape)\n",
    "\n",
    "loaded_ims = [img]\n",
    "\n",
    "\n",
    "im_batches = list(map(prep_image, loaded_ims, [inp_dim for x in range(len(imlist))]))\n",
    "\n",
    "\n",
    "im_dim_list = [(x.shape[1], x.shape[0]) for x in loaded_ims]\n",
    "im_dim_list = torch.FloatTensor(im_dim_list).repeat(1,2)\n",
    "print(im_dim_list)\n",
    "\n",
    "leftover = 0\n",
    "if (len(im_dim_list) % batch_size):\n",
    "    leftover = 1\n",
    "\n",
    "if batch_size != 1:\n",
    "    num_batches = len(imlist) // batch_size + leftover            \n",
    "    im_batches = [torch.cat((im_batches[i*batch_size : min((i +  1)*batch_size,\n",
    "                        len(im_batches))]))  for i in range(num_batches)]  \n",
    "\n",
    "write = 0\n",
    "\n",
    "if CUDA:\n",
    "    im_dim_list = im_dim_list.cuda()\n",
    "    \n",
    "start_det_loop = time.time()\n",
    "for i, batch in enumerate(im_batches):\n",
    "    # Load the image \n",
    "    start = time.time()\n",
    "    if CUDA:\n",
    "        batch = batch.cuda()\n",
    "    with torch.no_grad():\n",
    "        prediction = yolo_v4(Variable(batch))\n",
    "\n",
    "    prediction = write_results(prediction, confidence, num_classes, nms_conf = nms_thesh)\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    if type(prediction) == int:\n",
    "\n",
    "        for im_num, image in enumerate(imlist[i*batch_size: min((i +  1)*batch_size, len(imlist))]):\n",
    "            im_id = i*batch_size + im_num\n",
    "            print(\"{0:20s} predicted in {1:6.3f} seconds\".format(image.split(\"/\")[-1], (end - start)/batch_size))\n",
    "            print(\"{0:20s} {1:s}\".format(\"Objects Detected:\", \"\"))\n",
    "            print(\"----------------------------------------------------------\")\n",
    "        continue\n",
    "\n",
    "    prediction[:,0] += i*batch_size    #transform the atribute from index in batch to index in imlist \n",
    "\n",
    "    if not write:                      #If we have't initialised output\n",
    "        output = prediction  \n",
    "        write = 1\n",
    "    else:\n",
    "        output = torch.cat((output,prediction))\n",
    "\n",
    "    for im_num, image in enumerate(imlist[i*batch_size: min((i +  1)*batch_size, len(imlist))]):\n",
    "        im_id = i*batch_size + im_num\n",
    "        objs = [classes[int(x[-1])] for x in output if int(x[0]) == im_id]\n",
    "        print(\"{0:20s} predicted in {1:6.3f} seconds\".format(image.split(\"/\")[-1], (end - start)/batch_size))\n",
    "        # print(\"{0:20s} {1:s}\".format(\"Objects Detected:\", \" \".join(objs)))\n",
    "        print(\"----------------------------------------------------------\")\n",
    "\n",
    "    if CUDA:\n",
    "        torch.cuda.synchronize()       \n",
    "try:\n",
    "    output\n",
    "except NameError:\n",
    "    print (\"No detections were made\")\n",
    "    exit()\n",
    "\n",
    "im_dim_list = torch.index_select(im_dim_list, 0, output[:,0].long())\n",
    "\n",
    "scaling_factor = torch.min(yolo_v4.net_info[\"height\"]/im_dim_list,1)[0].view(-1,1)\n",
    "\n",
    "output[:,[1,3]] -= (inp_dim - scaling_factor*im_dim_list[:,0].view(-1,1))/2\n",
    "output[:,[2,4]] -= (inp_dim - scaling_factor*im_dim_list[:,1].view(-1,1))/2\n",
    "\n",
    "output[:,1:5] /= scaling_factor\n",
    "\n",
    "for i in range(output.shape[0]):\n",
    "    output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim_list[i,0])\n",
    "    output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim_list[i,1])\n",
    "    \n",
    "output_recast = time.time()\n",
    "class_load = time.time()\n",
    "colors = [[255, 0, 0], [255, 0, 0], [255, 255, 0], [0, 255, 0], [0, 255, 255], [0, 0, 255], [255, 0, 255]]\n",
    "\n",
    "draw = time.time()\n",
    "\n",
    "def write(x, results):\n",
    "    # Extract coordinates and class label from YOLOv4 output\n",
    "    c1 = tuple(map(int, x[1:3]))  # Top-left corner (x_min, y_min)\n",
    "    c2 = tuple(map(int, x[3:5]))  # Bottom-right corner (x_max, y_max)\n",
    "    \n",
    "    img = results[int(x[0])]  # Get the corresponding image from results list\n",
    "    \n",
    "    # Ensure image is a NumPy array\n",
    "    if img is None or len(img.shape) != 3:\n",
    "        print(\"Invalid image!\")\n",
    "        return img\n",
    "    \n",
    "    # Get the predicted class and corresponding label\n",
    "    cls = int(x[-1])\n",
    "    label = classes[cls]  # Get class label from classes list\n",
    "    \n",
    "    # Pick a random color for the bounding box\n",
    "    color = random.choice(colors)\n",
    "    \n",
    "    # Draw the rectangle around the detected object\n",
    "    cv2.rectangle(img, c1, c2, color, 2)  # Draw rectangle with thickness 2\n",
    "    \n",
    "    # Get the size of the label text to fit the rectangle\n",
    "    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1, 1)[0]\n",
    "    \n",
    "    # Adjust the bottom-right corner to fit the label text\n",
    "    c2 = (c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4)\n",
    "    \n",
    "    # Draw a filled rectangle for the label background\n",
    "    cv2.rectangle(img, c1, c2, color, -1)\n",
    "    \n",
    "    # Put the label text on top of the rectangle\n",
    "    cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225, 255, 255], 1)\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "list(map(lambda x: write(x, loaded_ims), output))\n",
    "\n",
    "det_names = pd.Series(imlist).apply(lambda x: \"{}/det_{}\".format(\"des\",x.split(\"/\")[-1]))\n",
    "\n",
    "list(map(cv2.imwrite, det_names, [cv2.cvtColor(loaded_ims[0], cv2.COLOR_BGR2RGB)]))\n",
    "end = time.time()\n",
    "\n",
    "print(\"SUMMARY\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"{:25s}: {}\".format(\"Task\", \"Time Taken (in seconds)\"))\n",
    "print()\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Reading addresses\", load_batch - read_dir))\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Loading batch\", start_det_loop - load_batch))\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Detection (\" + str(len(imlist)) +  \" images)\", output_recast - start_det_loop))\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Output Processing\", class_load - output_recast))\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Drawing Boxes\", end - draw))\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Average time_per_img\", (end - load_batch)/len(imlist)))\n",
    "print(\"----------------------------------------------------------\")\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88276acf-1870-4285-a52e-23d69679a4a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 44\u001b[0m\n\u001b[1;32m     40\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mputText(img, label, (c1[\u001b[38;5;241m0\u001b[39m], c1[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m t_size[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m4\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_PLAIN, \u001b[38;5;241m1\u001b[39m, [\u001b[38;5;241m225\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaded_ims\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m det_names \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(imlist)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/det_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdes\u001b[39m\u001b[38;5;124m\"\u001b[39m,x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(cv2\u001b[38;5;241m.\u001b[39mimwrite, det_names, [cv2\u001b[38;5;241m.\u001b[39mcvtColor(loaded_ims[\u001b[38;5;241m0\u001b[39m], cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)]))\n",
      "Cell \u001b[0;32mIn[22], line 44\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     40\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mputText(img, label, (c1[\u001b[38;5;241m0\u001b[39m], c1[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m t_size[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m4\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_PLAIN, \u001b[38;5;241m1\u001b[39m, [\u001b[38;5;241m225\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaded_ims\u001b[49m\u001b[43m)\u001b[49m, output))\n\u001b[1;32m     46\u001b[0m det_names \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(imlist)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/det_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdes\u001b[39m\u001b[38;5;124m\"\u001b[39m,x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(cv2\u001b[38;5;241m.\u001b[39mimwrite, det_names, [cv2\u001b[38;5;241m.\u001b[39mcvtColor(loaded_ims[\u001b[38;5;241m0\u001b[39m], cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)]))\n",
      "Cell \u001b[0;32mIn[22], line 10\u001b[0m, in \u001b[0;36mwrite\u001b[0;34m(x, results)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite\u001b[39m(x, results):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Extract coordinates and class label from YOLOv4 output\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     c1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Top-left corner (x_min, y_min)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     c2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, x[\u001b[38;5;241m3\u001b[39m:\u001b[38;5;241m5\u001b[39m]))  \u001b[38;5;66;03m# Bottom-right corner (x_max, y_max)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     img \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;28mint\u001b[39m(x[\u001b[38;5;241m0\u001b[39m])]  \u001b[38;5;66;03m# Get the corresponding image from results list\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import cv2\n",
    "\n",
    "# Assuming 'colors' and 'classes' are already defined somewhere\n",
    "# Example: colors = [(0, 255, 0), (255, 0, 0), ...]\n",
    "# Example: classes = [\"person\", \"car\", \"dog\", ...]\n",
    "\n",
    "def write(x, results):\n",
    "    # Extract coordinates and class label from YOLOv4 output\n",
    "    c1 = tuple(map(int, x[1:3]))  # Top-left corner (x_min, y_min)\n",
    "    c2 = tuple(map(int, x[3:5]))  # Bottom-right corner (x_max, y_max)\n",
    "    \n",
    "    img = results[int(x[0])]  # Get the corresponding image from results list\n",
    "    \n",
    "    # Ensure image is a NumPy array\n",
    "    if img is None or len(img.shape) != 3:\n",
    "        print(\"Invalid image!\")\n",
    "        return img\n",
    "    \n",
    "    # Get the predicted class and corresponding label\n",
    "    cls = int(x[-1])\n",
    "    label = classes[cls]  # Get class label from classes list\n",
    "    \n",
    "    # Pick a random color for the bounding box\n",
    "    color = random.choice(colors)\n",
    "    \n",
    "    # Draw the rectangle around the detected object\n",
    "    cv2.rectangle(img, c1, c2, color, 2)  # Draw rectangle with thickness 2\n",
    "    \n",
    "    # Get the size of the label text to fit the rectangle\n",
    "    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1, 1)[0]\n",
    "    \n",
    "    # Adjust the bottom-right corner to fit the label text\n",
    "    c2 = (c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4)\n",
    "    \n",
    "    # Draw a filled rectangle for the label background\n",
    "    cv2.rectangle(img, c1, c2, color, -1)\n",
    "    \n",
    "    # Put the label text on top of the rectangle\n",
    "    cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225, 255, 255], 1)\n",
    "    \n",
    "    return img\n",
    "\n",
    "list(map(lambda x: write(x, loaded_ims), output))\n",
    "\n",
    "det_names = pd.Series(imlist).apply(lambda x: \"{}/det_{}\".format(\"des\",x.split(\"/\")[-1]))\n",
    "\n",
    "list(map(cv2.imwrite, det_names, [cv2.cvtColor(loaded_ims[0], cv2.COLOR_BGR2RGB)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "487f4cc2-8a3a-433e-872d-2bbaa21fdf7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., nan, nan,  ..., nan, nan, 0.],\n",
       "        [0., nan, nan,  ..., nan, nan, 0.],\n",
       "        [0., nan, nan,  ..., nan, nan, 0.],\n",
       "        ...,\n",
       "        [0., nan, nan,  ..., nan, nan, 0.],\n",
       "        [0., nan, nan,  ..., nan, nan, 0.],\n",
       "        [0., nan, nan,  ..., nan, nan, 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
